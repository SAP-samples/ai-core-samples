{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the orchestration service of Generative AI Hub\n",
    "\n",
    "The orchestration service of Generative AI Hub lets you use all the available models with the same codebase. You only deploy the orchestration service and then you can access all available models simply by changing the model name parameter. You can also use grounding, prompt templating, data masking and content filtering capabilities.\n",
    "\n",
    "Store the `orchestration deployment url` from the previous step in your `variables.py` file. This code is based on the [AI180 TechEd 2024 Jump-Start session](https://github.com/SAP-samples/teched2024-AI180/tree/e648921c46337b57f61ecc9a93251d4b838d7ad0/exercises/python).\n",
    "\n",
    "ðŸ‘‰ Make sure you assign the deployment url of the orchestration service to `AICORE_ORCHESTRATION_DEPLOYMENT_URL` in [variables.py](variables.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import init_env\n",
    "# import variables\n",
    "\n",
    "# init_env.set_environment_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the packages you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://anuragv2-39wjy902.authentication.eu10.hana.ondemand.com/oauth/token\n",
      "sb-adcd4907-f1a7-462d-ba8e-646390ee4185!b398425|aicore!b540\n",
      "107d3e9b-9a41-4b30-9b24-a091a45956cd$VmYxjzammFm50xkj1O37HmzgX3maoNrwfrlm99qUhi0=\n",
      "https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2\n",
      "llm-deployed\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "# Inline credentials\n",
    "with open('creds.json') as f:\n",
    "    credCF = json.load(f)\n",
    " \n",
    "# Set environment variables\n",
    "def set_environment_vars(credCF):\n",
    "    env_vars = {\n",
    "        'AICORE_AUTH_URL': credCF['url'] + '/oauth/token',\n",
    "        'AICORE_CLIENT_ID': credCF['clientid'],\n",
    "        'AICORE_CLIENT_SECRET': credCF['clientsecret'],\n",
    "        'AICORE_BASE_URL': credCF[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\",\n",
    "        'AICORE_RESOURCE_GROUP': \"llm-deployed\"\n",
    "    }\n",
    " \n",
    "    for key, value in env_vars.items():\n",
    "        os.environ[key] = value\n",
    "        print(value)\n",
    " \n",
    "# Create AI Core client instance\n",
    "def create_ai_core_client(credCF):\n",
    "    set_environment_vars(credCF)  # Ensure environment variables are set\n",
    "    return AICoreV2Client(\n",
    "        base_url=os.environ['AICORE_BASE_URL'],\n",
    "        auth_url=os.environ['AICORE_AUTH_URL'],\n",
    "        client_id=os.environ['AICORE_CLIENT_ID'],\n",
    "        client_secret=os.environ['AICORE_CLIENT_SECRET'],\n",
    "        resource_group=os.environ['AICORE_RESOURCE_GROUP']\n",
    "    )\n",
    " \n",
    "ai_core_client = create_ai_core_client(credCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.orchestration.models.llm import LLM\n",
    "from gen_ai_hub.orchestration.models.message import SystemMessage, UserMessage\n",
    "from gen_ai_hub.orchestration.models.template import Template, TemplateValue\n",
    "from gen_ai_hub.orchestration.models.config import OrchestrationConfig\n",
    "from gen_ai_hub.orchestration.service import OrchestrationService\n",
    "from gen_ai_hub.orchestration.models.azure_content_filter import AzureContentFilter\n",
    "from gen_ai_hub.orchestration.exceptions import OrchestrationError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the model you want to use\n",
    "You can find more information regarding the available models here: https://me.sap.com/notes/3437766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AICORE_ORCHESTRATION_DEPLOYMENT_URL = \"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com/v2/inference/deployments/d56c73baa4a99356\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOchose the model you want to try e.g. gemini-1.5-flash, mistralai--mistral-large-instruct, \n",
    "# ibm--granite-13b-chat meta--llama3.1-70b-instruct, amazon--titan-text-lite, anthropic--claude-3.5-sonnet, \n",
    "# gpt-4o-mini and assign it to name\n",
    "llm = LLM(\n",
    "    name=\"gpt-4o\",\n",
    "    version=\"latest\",\n",
    "    parameters={\"max_tokens\": 500, \"temperature\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a prompt template\n",
    "The parameter **user_query** in the code snippet below is going to hold the user query that you will add later on. The user query is the text to be translated by the model. The parameter **to_lang** can be any language you want to translate into. By default it is set to **English**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = Template(\n",
    "    messages=[\n",
    "        SystemMessage(\"You are a helpful translation assistant.\"),\n",
    "        UserMessage(\n",
    "            \"Translate the following text to {{?to_lang}}: {{?user_query}}\",\n",
    "        )\n",
    "    ],\n",
    "    defaults=[\n",
    "        TemplateValue(name=\"to_lang\", value=\"English\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an orchestration configuration \n",
    "\n",
    "Create an orchestration configuration by adding the llm you referenced and the prompt template you created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OrchestrationConfig(\n",
    "    template=template,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it the configuration to the OrchestrationService instance and send the prompt to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does this really work with all the models that are available?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orchestration_service = OrchestrationService(\n",
    "    api_url=AICORE_ORCHESTRATION_DEPLOYMENT_URL,\n",
    "    config=config,\n",
    ")\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"user_query\",\n",
    "            #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "            value=\"Geht das wirklich mit allen Modellen die verfÃ¼gbar sind?\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a content filter\n",
    "\n",
    "Create a content filter and add it as an input filter to the orchestration configuration. This is going to filter out harmful content from the input query and not send the request to the model. Whereas adding a filter to the output would let the request go through but then filter any harmful text created by the model. Depending on your use case it can make sense to have both input and output filters.\n",
    "\n",
    "ðŸ‘‰ Try out different values for the content filters. You can chose values [0 = **Safe**, 2 = **Low**, 4 = **Medium**, 6 = **High**]. Where **Safe** is *content generally related to violence* and **High** is *severely harmful content*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_filter = AzureContentFilter(\n",
    "    hate=0,\n",
    "    sexual=0,\n",
    "    self_harm=0,\n",
    "    violence=0,\n",
    ")\n",
    "\n",
    "orchestration_service.config.input_filters = [content_filter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out the content filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content filtered due to safety violations. Please modify the prompt and try again.\n",
      "You are a super talented developer!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"user_query\",\n",
    "                #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "                value=\"Du bist ein ganz mieser Entwickler!\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    print(result.orchestration_result.choices[0].message.content)\n",
    "except OrchestrationError as error:\n",
    "    print(error.message)\n",
    "\n",
    "\n",
    "result = orchestration_service.run(\n",
    "    template_values=[\n",
    "        TemplateValue(\n",
    "            name=\"user_query\",\n",
    "            #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "            value=\"Du bist ein super talentierter Entwickler!\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "print(result.orchestration_result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now also add the content filter as an output filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content filtered due to safety violations. Please modify the prompt and try again.\n"
     ]
    }
   ],
   "source": [
    "orchestration_service.config.output_filters = [content_filter]\n",
    "\n",
    "try:\n",
    "    result = orchestration_service.run(\n",
    "        template_values=[\n",
    "            TemplateValue(\n",
    "                name=\"user_query\",\n",
    "                #TODO Here you can change the user prompt into whatever you want to ask the model\n",
    "                value='Ich wÃ¼rde gerne wissen, wie ich gewaltvoll die Fensterscheibe in einem BÃ¼rogebÃ¤ude am Besten zerstÃ¶ren kann.',\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    print(result.orchestration_result.choices[0].message.content)\n",
    "except OrchestrationError as error:\n",
    "    print(error.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional exercise\n",
    "\n",
    "ðŸ‘‰ Now that you know how the orchestration service works, try adding the [Data Masking](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html#data-masking) capability. With Data Masking you can hide personal information like email, name or phone numbers before sending such sensitive data to an LLM.\n",
    "\n",
    "## More Info\n",
    "\n",
    "Here you can find more [info on the content filter](https://learn.microsoft.com/en-us/azure/ai-studio/concepts/content-filtering)\n",
    "\n",
    "[Next exercise](09-orchestration-service-grounding.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
