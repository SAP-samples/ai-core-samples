{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3defa59-142d-4906-add5-b809d01d126b",
   "metadata": {},
   "source": [
    "## Utilizing RAG Embeddings for Enhanced Developer Support\n",
    "\n",
    "Bob is a Content  developer in the XYZ company. His team works on a node library that can simplify the software development process inside and outside the company. His team has designed a dedicated documentation for the library. But the developers who use the library always face issues and contact Bob. To make his job easy, Bob decided to use Generative AI for this Use case. \n",
    "\n",
    "Bob uses the Generative AI Hub SDK to create a help portal for his library. He wants to Improve the effectiveness of our help portal by integrating RAG (Retrieval-Augmented Generation) embeddings so that Developers can receive more contextually relevant responses to their queries about the node library. \n",
    "\n",
    "Acceptance criteria: \n",
    "\n",
    "- Bob utilizes the Generative AI Hub SDK to integrate a generative AI model into the help portal for the company's node library. \n",
    "\n",
    "- Bob configures and deploys the generative AI model within the AI core instance of the company's infrastructure. \n",
    "\n",
    "- The help portal powered by Generative AI should provide intuitive and contextually relevant responses to developers' inquiries about using the node library. \n",
    "\n",
    "- The generative AI model is trained on the documentation of the node library to ensure accurate and informative responses. \n",
    "\n",
    "- The help portal interface is user-friendly, allowing developers to easily interact with the generative AI model and find answers to their questions. \n",
    "\n",
    "- Bob monitors the performance of the generative AI model regularly, ensuring that it continues to provide accurate and helpful responses to developers' queries. \n",
    "\n",
    "- Bob documents the process of integrating the Generative AI Help Portal into the company's support workflow, providing clear instructions for future maintenance and updates. \n",
    "\n",
    "- Bob Queries the model \n",
    "\n",
    "- Implements RAG embeddings \n",
    "\n",
    "- Enhances Query responses \n",
    "\n",
    "- Bob conducts training sessions or provides documentation to the support team, educating them on how to leverage the Generative AI Help Portal effectively to assist developers. \n",
    "\n",
    "### Setting up LLM's on AI core\n",
    "\n",
    "- Bob navigates to ML Operations -> Scenarios in the AI-Core workspace. \n",
    "\n",
    "- He ensures that the foundation-models scenario is present. \n",
    "\n",
    "- Bob goes to ML Operations -> Configurations and creates a new configuration: \n",
    "\n",
    "- Names it appropriately. \n",
    "\n",
    "- Selects the foundation-models scenario. \n",
    "\n",
    "- Chooses the version and executable ID. \n",
    "\n",
    "- He sets input parameters specifying the model name and version. \n",
    "\n",
    "- Bob reviews and creates the configuration. \n",
    "\n",
    "Model Deployment: \n",
    "\n",
    "- Bob creates a deployment for the configured model, setting the duration to standard. \n",
    "\n",
    "- After the deployment status changes to RUNNING, he verifies its availability. \n",
    "\n",
    "Model Integration: \n",
    "\n",
    "- Bob installs the Generative AI Hub SDK by running pip3 install generative-ai-hub-sdk. \n",
    "\n",
    "- He configures the SDK using AI core keys stored in the local directory. \n",
    "\n",
    "\n",
    "\n",
    "- Bob uses Python code to generate responses for queries using the SDK. \n",
    "\n",
    "- He formats the query and invokes the Generative AI Hub SDK to fetch responses. \n",
    "\n",
    "Implementing RAG Embeddings: \n",
    "\n",
    "- Bob prepares the documentation for the node library in CSV format with each row representing a topic. \n",
    "\n",
    "- He connects to the HANA vector storage instance and creates a table to store the documentation data. \n",
    "\n",
    "- Bob populates the table with data and creates a REAL_VECTOR column to store embeddings. \n",
    "\n",
    "- Using the Generative AI Hub SDK, he defines a function to generate embeddings for prompts and performs similarity search using the embeddings. \n",
    "\n",
    "Enhancing Query Responses: \n",
    "\n",
    "- Bob defines a prompt template to provide context to queries. \n",
    "\n",
    "- He modifies the function to query the LLM (Large Language Model) based on the prompt template. \n",
    "\n",
    "- Bob tests the model's response using specific queries related to the node library, ensuring it provides contextually relevant responses based on embeddings.  Bob installs Jupyter Notebook using pip3 install notebook. \n",
    "\n",
    "Retrieval augmented generation optimizes the output of large language models by giving more contexts to your prompts.\n",
    "\n",
    "In this workshop, we use a [graph document](files/GRAPH_DOCU_2503.csv) to give more context to the model, thus optimiz\n",
    "iLoad the document into the data variable. is a structure that contains two-dimensional data and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain openai docarray panel jupyter_bokeh redlines tiktoken wikipedia DateTime presidio_anonymizer hana-ml pandas presidio_analyzer\n",
    "!pip3 install generative-ai-hub-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e59c835-d853-429a-bbbf-78d1c9894586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('GRAPH_DOCU_2503.csv', encoding='utf-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            data.append(row)\n",
    "        except:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe7f05-ae81-46b2-a9bd-ec4eddc31f6a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we can create a connection to the HANA Vector storage. Replace the address, port-number, username, and password from the values in your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417f80d2-29e4-4fc1-8b59-1304425bf2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbcli\n",
    "from hdbcli import dbapi\n",
    "\n",
    "cc = dbapi.connect(\n",
    "    address='4e77f46b-5af4-4cdf-96b6-2c7cae6ddb39.hana.prod-eu10.hanacloud.ondemand.com',\n",
    "    port='443',\n",
    "    user='DBADMIN',\n",
    "    password='Initial1',\n",
    "    encrypt=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8ea5e-0ae7-442e-8f69-97e22cee88bf",
   "metadata": {},
   "source": [
    "Now we have created a connection to the HANA vector storage. This connection will help us to work with the vector storage in the upcoming steps.\n",
    "\n",
    "HANA vector storage stores data in form of tables. To store our data, we can create a table in our HANA vector storage.\n",
    "\n",
    "Replace the TABLENAME with a name of your choice. **Make sure that you use only Uppercase letters and numbers in the tablename**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e535a-1997-42f9-a56b-822d0f6d3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table\n",
    "cursor = cc.cursor()\n",
    "sql_command = '''CREATE TABLE TEST06(ID1 BIGINT, ID2 BIGINT, L1 NVARCHAR(3), L2 NVARCHAR(3), L3 NVARCHAR(3), FILENAME NVARCHAR(100), HEADER1 NVARCHAR(5000), HEADER2 NVARCHAR(5000), TEXT NCLOB, VECTOR_STR REAL_VECTOR);'''\n",
    "cursor.execute(sql_command)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968b049-c95e-4d3d-92f2-df853e66fce9",
   "metadata": {},
   "source": [
    "In the SQL command, we are creating a table called 'GRAPH_DOCU_2503'. You can give any name of your choice, but make sure that you use the same table name throughout. Also keep in mind that you cannot create multiple tables with the same name.\n",
    "\n",
    "Once you have created the table, you can populate the table with our data which we have converted to the dataframe in the previous step.\n",
    "\n",
    "Replace the TABLENAME with the name of your table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641461c5-1c5d-45b5-999a-805816b3ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cc.cursor()\n",
    "sql_insert = 'INSERT INTO TEST06(ID1, ID2, L1, L2, L3, FILENAME, HEADER1, HEADER2, TEXT, VECTOR_STR) VALUES (?,?,?,?,?,?,?,?,?,TO_REAL_VECTOR(?))'\n",
    "cursor.executemany(sql_insert,data[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efc76b-5542-425e-a722-567f2ce12d63",
   "metadata": {},
   "source": [
    "\n",
    "Embeddings are vector representations of text data that incorporates the semantic meaning of the text. Define a get_embedding() function that generates embeddings from text data using the text-embedding-ada-002 model. This function will be used to generate embeddings from the user's prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1d1974-c692-4b30-8a2a-a044f71fa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "def get_embedding(input, model=\"text-embedding-ada-002\") -> str:\n",
    "    response = embeddings.create(\n",
    "      model_name=model,\n",
    "      input=input\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dcfd7-ffa9-4d46-a66e-5a673473174d",
   "metadata": {},
   "source": [
    "\n",
    "The embeddings can be used to find texts that are similar to each other using techniques such as COSINE-SIMILARITY. You can use such techniques to find text data similar to your prompt from the HANA vector store.\n",
    "\n",
    "Now define a function to do similarity search in your data. This function takes a query as input and returns similar content from the table.\n",
    "\n",
    "Replace the TABLENAME with the name of your table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "548537af-f2cb-4955-ba39-4d9407ad9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cc.cursor()\n",
    "def run_vector_search(query: str, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    if metric == 'L2DISTANCE':\n",
    "        sort = 'ASC'\n",
    "    else:\n",
    "        sort = 'DESC'\n",
    "    query_vector = get_embedding(query)\n",
    "    sql = '''SELECT TOP {k} \"ID2\", \"TEXT\"\n",
    "        FROM \"TEST06\"\n",
    "        ORDER BY \"{metric}\"(\"VECTOR_STR\", TO_REAL_VECTOR('{qv}')) {sort}'''.format(k=k, metric=metric, qv=query_vector, sort=sort)\n",
    "    cursor.execute(sql)\n",
    "    hdf = cursor.fetchall()\n",
    "    return hdf[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94990eca-90d2-4aa2-8833-a3187c47aeba",
   "metadata": {},
   "source": [
    "We can define a prompt template to provide context to our prompts. Thus, when we pass a prompt to the model, the template will add the necessary context to the prompt so that better results are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd39bd7c-0234-4b3f-9bd3-0329524ed18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate_fstring = \"\"\"\n",
    "You are an SAP HANA Cloud expert.\n",
    "You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "promptTemplate = PromptTemplate.from_template(promptTemplate_fstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd6bd2-1443-430c-8248-febc27512bdd",
   "metadata": {},
   "source": [
    "Here we have created a template for the prompts. It contains two variables, 'context' and 'query'. These variables will be replaced with the context and query in the upcoming steps \n",
    "\n",
    "Now we can define a function ask_llm() that queries a model using the above template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9c7f4b-a42f-462d-a2bb-a77f8ba41bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\slcib\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def ask_llm(query: str, metric='COSINE_SIMILARITY', k = 4) -> str:\n",
    "    context = ''\n",
    "    context = run_vector_search(query, metric, k)\n",
    "    prompt = promptTemplate.format(query=query, context=' '.join(str(context)))\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(str(prompt)))\n",
    "    print('no of tokens'+ str(num_tokens))\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k',max_tokens = 8000)\n",
    "    response = llm.invoke(prompt).content\n",
    "    print('Query: '+ query)\n",
    "    print('\\nResponse:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa8a66-8241-4e81-a7d9-c632239edbdf",
   "metadata": {},
   "source": [
    "The above function appends the query and context into the template to create a prompt. Then that prompt is passed on to the LLM model and the response is printed.\n",
    "\n",
    "Here we are using gpt-4 model. Make sure that you have already created a deployment for the gpt-4 model in AI Launchpad.\n",
    "\n",
    "We can compare how the output produced by RAG is different from the output when we directly pass the prompt to the model. If we directly pass the prompt to the model withouth RAG, this will be the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e617f819-e8fd-42ed-b5c1-b3679d3c187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Running a shortest path algorithm involves several steps, depending on the specific algorithm you\\'re using. However, let\\'s consider the Dijkstra\\'s algorithm, which is one of the most common. Here\\'s a simple way to run this algorithm in Python:\\n\\n1. Define your graph: This can be done using a dictionary. Each key will be a node, and its value will be a list of tuples, where each tuple represents a neighboring node and the cost to reach it.\\n\\n```python\\ngraph = {\\n    \\'A\\': [(\\'B\\', 1), (\\'C\\', 3), (\\'E\\', 7)],\\n    \\'B\\': [(\\'D\\', 2), (\\'E\\', 6)],\\n    \\'C\\': [(\\'D\\', 1), (\\'F\\', 3)],\\n    \\'D\\': [(\\'E\\', 1)],\\n    \\'E\\': [(\\'F\\', 1)],\\n    \\'F\\': []\\n}\\n```\\n2. Define Dijkstra\\'s algorithm:\\n\\n```python\\ndef dijkstra(graph, start, end):\\n    shortest_paths = {start: (None, 0)}\\n    current_node = start\\n    visited = set()\\n\\n    while current_node != end:\\n        visited.add(current_node)\\n        destinations = graph[current_node]\\n        weight_to_current_node = shortest_paths[current_node][1]\\n\\n        for next_node, weight in destinations:\\n            weight = weight + weight_to_current_node\\n            if next_node not in shortest_paths:\\n                shortest_paths[next_node] = (current_node, weight)\\n            else:\\n                current_shortest_weight = shortest_paths[next_node][1]\\n                if current_shortest_weight > weight:\\n                    shortest_paths[next_node] = (current_node, weight)\\n\\n        next_destinations = {node: shortest_paths[node] for node in shortest_paths if node not in visited}\\n        if not next_destinations:\\n            return \"Route Not Possible\"\\n        current_node = min(next_destinations, key=lambda k: next_destinations[k][1])\\n\\n    path = []\\n    while current_node is not None:\\n        path.append(current_node)\\n        next_node = shortest_paths[current_node][0]\\n        current_node = next_node\\n    path = path[::-1]\\n    return path\\n```\\n\\n3. Run the algorithm:\\n\\n```python\\nprint(dijkstra(graph, \\'A\\', \\'F\\')) # Output: [\\'A\\', \\'B\\', \\'D\\', \\'E\\', \\'F\\']\\n```\\n\\nThis will print the shortest path from \\'A\\' to \\'F\\'. The list represents the nodes visited in order, forming the shortest path.\\n\\nPlease note that this is a very basic implementation of Dijkstra\\'s algorithm and may not work for more complex cases. For example, it doesn\\'t handle negative weights. For a more robust solution, consider using a library like NetworkX that has built-in functions for shortest path algorithms.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How can I run a shortest path algorithm?\"\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "response = llm.invoke(query).content\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4ffbe-e081-4f7c-80b8-5a5e2f54da30",
   "metadata": {},
   "source": [
    "Now we can test the function by passing the following query to generate results using retrieval augmented generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b61ff1a-6d15-4f2e-a17a-eb51d02c6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens12880\n",
      "Query: I want to calculate a shortest path. How do I do that?\n",
      "\n",
      "Response:\n",
      "To calculate the shortest path in a weighted graph, you can use Dijkstra's algorithm. In SAP HANA, this can be done using the GraphScript language. \n",
      "\n",
      "Here is an example:\n",
      "```graphscript\n",
      "TRAVERSE DIJKSTRA :g FROM :startVertex\n",
      "WITH WEIGHT (EDGE e) => DOUBLE {\n",
      "   return :e.\"weight\";\n",
      "}\n",
      "ON VISIT VERTEX (VERTEX v, DOUBLE distance) {\n",
      "   v.\"distance\" = :distance;\n",
      "};\n",
      "```\n",
      "In this example, `:g` represents the graph, `:startVertex` is the starting vertex. The `WITH WEIGHT` clause calculates the weight of each edge in the graph, which is passed as a parameter and returns the weight value. The `ON VISIT VERTEX` hook stores the shortest distance of each visited vertex in a temporary vertex attribute.\n",
      "\n",
      "Another way to calculate the shortest path is by using the built-in function `SHORTEST_PATH` in GraphScript. \n",
      "Here is an example:\n",
      "```bnf\n",
      "<sssp_function> ::= SHORTEST_PATH '( '<parent_graph_variable>,\n",
      "<source_vertex>, <target_vertex> [ , <clos_expr> ] [ , <edge_direction> ] ')'\n",
      "```\n",
      "This function returns a `WEIGHTEDPATH` instance containing a shortest path within the given parent graph from a start vertex to a target vertex. If no path exists from the source vertex to the target vertex, an empty path is returned. \n",
      "\n",
      "You can also use custom weight functions as a closure expression to specify your own weight metric for the `SHORTEST_PATH` function. The weight function takes an edge, current weight distance, and hop distance as parameters and returns a value of a numerical type.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"I want to calculate a shortest path. How do I do that?\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bdada-6fb3-45dd-b314-4027b1beef71",
   "metadata": {},
   "source": [
    "Here we can see that we get better output when we use RAG\n",
    "\n",
    "**Outcome:** By implementing RAG embeddings in the help portal, Bob enhances the support experience for developers using the node library. The help portal now offers more accurate and contextual responses to queries, reducing dependency on manual support and empowering developers to find solutions independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3870db-1e0d-45f9-9e28-0fb51d9f5698",
   "metadata": {},
   "source": [
    "## Prompting the model\n",
    "\n",
    "The LLM can do a wide variety of tasks using prompting techniques. By efficient use of prompts, the models can achieve good results in the following tasks\n",
    "\n",
    "**Content creation using RAG**\n",
    "\n",
    "The model can generate text content for articles, blogs, stories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc31aec6-e78f-45d1-be84-1369686e9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens2469\n",
      "Query: Write an article on the new features in graphs.\n",
      "\n",
      "Response:\n",
      "Title: New Features in Graphs with SAP HANA Cloud\n",
      "\n",
      "Introduction\n",
      "\n",
      "Graphs are highly versatile and powerful abstractions that can model various types of networks and linked data. In recent years, they have found extensive applications across numerous industries, including logistics and transportation, utility networks, knowledge representation, and text processing. SAP HANA Graph is an integral part of the SAP HANA core functionality and expands the platform with native support for graph processing. This article explores the new features in graphs that come with SAP HANA.\n",
      "\n",
      "GraphScript Grammar\n",
      "\n",
      "The latest SAP HANA Graph has introduced the GraphScript grammar. GraphScript is a powerful, flexible language with basic elements and supports variables. It provides options for specifying the control flow, thus making it easier to define complex graph operations. The language allows for one-line comments as well as multi-line comments, enhancing the readability and maintainability of the code.\n",
      "\n",
      "Graph Structure and Tables\n",
      "\n",
      "A graph in SAP HANA Cloud consists of vertices and edges. Vertices are the entities in the graph, and edges describe the relationships between these vertices. Each edge connects two vertices, one being the source and the other the target. Edges are always directed but can be navigated in either direction. An arbitrary number of attributes can be associated with vertices and edges. Vertices and edges are stored in their respective tables, collectively referred to as graph tables. These tables can be any relational object of type table, view, or synonym, providing a high level of flexibility.\n",
      "\n",
      "Attributes and Associated Labels\n",
      "\n",
      "Every vertex and edge in a graph can have an associated label, representing the role in the modeled domain. Vertex attributes correspond to the columns of the vertex tables, and edge attributes correspond to the columns of the edge tables. The maximum number of attributes is bound by the maximum number of columns for the underlying tables. This feature offers a high degree of customization, enabling users to define a wide range of attributes for vertices and edges. \n",
      "\n",
      "Relational Storage and Access\n",
      "\n",
      "SAP HANA Graph employs relational storage, which allows the entire feature set of SAP HANA to be applied to the graph data. This includes access control, backup, and recovery. All SAP HANA Graph functions can be applied to the graph data stored in relational format, making it highly compatible with business applications. \n",
      "\n",
      "Graph Workspaces\n",
      "\n",
      "SAP HANA Graph introduces graph workspaces as dedicated catalog objects for defining graphs. This feature provides an organized and structured environment for managing and processing graph data.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The new features in SAP HANA Graph significantly enhance the capabilities of graph processing. With the introduction of GraphScript, the flexible storage and attribute options, and dedicated graph workspaces, SAP HANA Cloud is providing a robust and versatile platform for working with graphs across a wide range of applications.\n"
     ]
    }
   ],
   "source": [
    "query = \"Write an article on the new features in graphs.\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357784ec-0951-4efc-9d85-22986ec5b43e",
   "metadata": {},
   "source": [
    "\n",
    "**Language Translation using RAG**\n",
    "\n",
    "If you want to get the output in another language, you can ask the model to do so in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86afecfb-32d4-48d3-a40f-d8f1f35d53e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens7780\n",
      "Query: How can I find the shortest distance between two nodes? Give the output in Dutch.\n",
      "\n",
      "Response:\n",
      "You can find the shortest distance between two nodes using Dijkstra's algorithm in SAP HANA Cloud. The graphscript example below demonstrates how to calculate the shortest distance between two nodes:\n",
      "\n",
      "```graphscript\n",
      "TRAVERSE DIJKSTRA :g FROM :startVertex\n",
      "WITH WEIGHT (EDGE e) => DOUBLE {\n",
      "return :e.\"weight\";\n",
      "}\n",
      "ON VISIT VERTEX (VERTEX v, DOUBLE distance) {\n",
      "v.\"distance\" = :distance;\n",
      "};\n",
      "```\n",
      "In this script, Dijkstra's algorithm is used to find the shortest path in a weighted graph. The vertices are visited in ascending order of their shortest distance from the start vertex. The weight of each edge is calculated using a mandatory WITH WEIGHT clause. The shortest distance of each visited vertex is stored in a temporary vertex attribute. \n",
      "\n",
      "Please note that the 'startVertex' must be a single vertex and the traversal directions supported are OUTGOING, INCOMING, and ANY. \n",
      "\n",
      "In Dutch:\n",
      "\n",
      "U kunt de kortste afstand tussen twee knooppunten vinden met behulp van Dijkstra's algoritme in SAP HANA Cloud. Het onderstaande graphscript-voorbeeld laat zien hoe u de kortste afstand tussen twee knooppunten kunt berekenen:\n",
      "\n",
      "```graphscript\n",
      "TRAVERSE DIJKSTRA :g FROM :startVertex\n",
      "MET GEWICHT (EDGE e) => DOUBLE {\n",
      "return :e.\"gewicht\";\n",
      "}\n",
      "BIJ BEZOEK VERTEX (VERTEX v, DOUBLE afstand) {\n",
      "v.\"afstand\" = :afstand;\n",
      "};\n",
      "```\n",
      "In dit script wordt het algoritme van Dijkstra gebruikt om het kortste pad in een gewogen grafiek te vinden. De knooppunten worden bezocht in oplopende volgorde van hun kortste afstand vanaf het startknooppunt. Het gewicht van elke rand wordt berekend met behulp van een verplichte MET GEWICHT-clausule. De kortste afstand van elk bezocht knooppunt wordt opgeslagen in een tijdelijk knooppuntattribuut.\n",
      "\n",
      "Let op: het 'startVertex' moet een enkel knooppunt zijn en de ondersteunde traverseringsrichtingen zijn UITGAAND, INKOMEND en ALLES.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I find the shortest distance between two nodes? Give the output in Dutch.\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42747555-60a1-45de-94fd-aaa29f729add",
   "metadata": {},
   "source": [
    "**Text Completion using RAG**\n",
    "\n",
    "The models can be used for text completions. By giving a text input, the models suggests completions for the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf016cb7-8bde-4ff2-a5c3-c36978175cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens3138\n",
      "Query: Graphs can be a useful tool for computer scientists when ...\n",
      "\n",
      "Response:\n",
      "... modeling different kinds of networks and linked data. They can come from various industries such as logistics and transportation, utility networks, knowledge representation, and text processing. In SAP HANA, GraphScript, an imperative programming language, provides developers with a high-level interface for accessing graph data. It simplifies the development of application-specific graph algorithms and integrates them into SQL-based data processing. GraphScript also offers optimized built-in algorithms to solve common graph-related problems, such as finding the shortest path from one vertex to another. Graphs in SAP HANA are made up of vertices and edges, stored in vertex and edge tables respectively, and can have arbitrary attributes. If the graph workspace is inconsistent, a GraphScript program will terminate with an error.\n"
     ]
    }
   ],
   "source": [
    "query = \"Graphs can be a useful tool for computer scientists when ...\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f78e44-1ab4-4ba6-8787-e3a632de4c16",
   "metadata": {},
   "source": [
    "**Text Classification using RAG**\n",
    "\n",
    "The models can be used to classify text based on their semantics. In the following example, the model can classify whether a question is related to graphs or related to business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fc730b-59a0-4f7b-9939-887e60d47ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens5776\n",
      "Query: Classify the following questions into graph related and business related questions. \n",
      "\n",
      "    1. How does diversification reduce investment risk?\n",
      "    2. Describe the role of dividends in stock investing.\n",
      "    3. What is the degree of a node in a graph, and how is it calculated?\n",
      "    4. What factors influence consumer behavior in marketing?\n",
      "    5. Explain the basic principles of the law of diminishing returns.\n",
      "    6. What is the concept of opportunity cost in economics?\n",
      "    7. What is the definition of a graph in graph theory?\n",
      "    8. How are directed and undirected graphs distinguished from each other?\n",
      "    9. Explain the difference between a path and a cycle in a graph.\n",
      "    10. What is a connected graph, and why is it significant in the study of graphs?\n",
      "\n",
      "Response:\n",
      "Graph related questions: \n",
      "\n",
      "    3. What is the degree of a node in a graph, and how is it calculated?\n",
      "    7. What is the definition of a graph in graph theory?\n",
      "    8. How are directed and undirected graphs distinguished from each other?\n",
      "    9. Explain the difference between a path and a cycle in a graph.\n",
      "    10. What is a connected graph, and why is it significant in the study of graphs?\n",
      "\n",
      "Business related questions:\n",
      "\n",
      "    1. How does diversification reduce investment risk?\n",
      "    2. Describe the role of dividends in stock investing.\n",
      "    4. What factors influence consumer behavior in marketing?\n",
      "    5. Explain the basic principles of the law of diminishing returns.\n",
      "    6. What is the concept of opportunity cost in economics?\n"
     ]
    }
   ],
   "source": [
    "query = '''Classify the following questions into graph related and business related questions. \n",
    "\n",
    "    1. How does diversification reduce investment risk?\n",
    "    2. Describe the role of dividends in stock investing.\n",
    "    3. What is the degree of a node in a graph, and how is it calculated?\n",
    "    4. What factors influence consumer behavior in marketing?\n",
    "    5. Explain the basic principles of the law of diminishing returns.\n",
    "    6. What is the concept of opportunity cost in economics?\n",
    "    7. What is the definition of a graph in graph theory?\n",
    "    8. How are directed and undirected graphs distinguished from each other?\n",
    "    9. Explain the difference between a path and a cycle in a graph.\n",
    "    10. What is a connected graph, and why is it significant in the study of graphs?'''\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867db6b-8679-4d62-bdae-8310191dcf58",
   "metadata": {},
   "source": [
    "\n",
    "**Tone adjustment using RAG**\n",
    "\n",
    "The models can adjust the tone of a content according to the instructions given. In the following example, we see a snippet from an article that has an anger tone, and we ask the LLM to make it professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f456cfae-2559-4da2-afd0-488511491c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens2648\n",
      "Query: \n",
      "    The following conent is not fit for a professional document. Rewrite the same in a professional tone. \n",
      "        \n",
      "    I can't believe we're still dealing with this nonsense! If you dare to mess around with the referenced graph tables without ensuring the consistency of the graph workspace through referential constraints, you're practically asking for chaos! And let me tell you, when that happens, each blasted Graph Engine component behaves differently! It's like dealing with a bunch of unruly children who can't agree on a single thing!\n",
      "\n",
      "    Check out the CREATE GRAPH WORKSPACE Statement and the DROP GRAPH WORKSPACE Statement. Maybe if you bothered to read them properly, you wouldn't be stuck in this mess! But no, here we are, dealing with your incompetence and the fallout of your reckless actions. Get your act together and start taking responsibility for maintaining a freaking consistent graph workspace!\n",
      "\n",
      "Response:\n",
      "It's critical to maintain the integrity of the referenced graph tables to ensure the consistency of the graph workspace. It's important to note that without the proper implementation of referential constraints, the system may experience undesirable outcomes. Different Graph Engine components may respond variably, resulting in potential inconsistencies.\n",
      "\n",
      "It's recommended to familiarize yourself with the CREATE GRAPH WORKSPACE Statement and the DROP GRAPH WORKSPACE Statement. By understanding and correctly implementing these statements, you can prevent many common issues from arising. It's essential to take responsibility for the maintenance of a consistent graph workspace to ensure smooth operations and avoid unnecessary complications.\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "    The following conent is not fit for a professional document. Rewrite the same in a professional tone. \n",
    "        \n",
    "    I can't believe we're still dealing with this nonsense! If you dare to mess around with the referenced graph tables without ensuring the consistency of the graph workspace through referential constraints, you're practically asking for chaos! And let me tell you, when that happens, each blasted Graph Engine component behaves differently! It's like dealing with a bunch of unruly children who can't agree on a single thing!\n",
    "\n",
    "    Check out the CREATE GRAPH WORKSPACE Statement and the DROP GRAPH WORKSPACE Statement. Maybe if you bothered to read them properly, you wouldn't be stuck in this mess! But no, here we are, dealing with your incompetence and the fallout of your reckless actions. Get your act together and start taking responsibility for maintaining a freaking consistent graph workspace!'''\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1709b16-1ec7-44a5-9b47-0fe787f0a607",
   "metadata": {},
   "source": [
    "\n",
    "**Spell Check / Grammar Check using RAG**\n",
    "\n",
    "The model can also detect the spelling and grammatical issues in a document and correct them promptly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e691c9b-7c20-45a7-82d0-99f4a7079a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of tokens1788\n",
      "Query: \n",
      "    Proofread and rewrite the following sentence:\n",
      "    The GraphScript langaguae  will eases the developomont of application-specific graph algthrihms and integrates them into SQL-based data procezzing.\n",
      "        \n",
      "\n",
      "Response:\n",
      "The GraphScript language eases the development of application-specific graph algorithms and integrates them into SQL-based data processing.\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "    Proofread and rewrite the following sentence:\n",
    "    The GraphScript langaguae  will eases the developomont of application-specific graph algthrihms and integrates them into SQL-based data procezzing.\n",
    "        '''\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00538abf-712f-46f4-a6ad-dc00a8c3412c",
   "metadata": {},
   "source": [
    "## Using data from reddit for Retrieval Augmented Generation\n",
    "\n",
    "As a Content Developer at XYZ Company, Bob's journey to improve developer support with embeddings encounters a new challenge when Alice, a colleague in his team, discovers critical information on Reddit. Here's how it integrates into the user story: \n",
    "\n",
    "- Discovery of Critical Information: \n",
    "\n",
    "    - Alice notices a Reddit post highlighting issues with a deprecated function in the node library. Recognizing the potential impact on developers, she investigates and finds a workaround, which she promptly shares as a reply to the post. \n",
    "\n",
    "- Increased Support Queries: \n",
    "     - The following day, Bob's team receives numerous inquiries related to the same issue discussed on Reddit. Despite Alice's timely response on the platform, developers continue to face challenges, leading to an influx of support queries. \n",
    "\n",
    "- Addressing Model Limitations: \n",
    "\n",
    "    - Bob identifies a gap between the support provided by their existing model and the real-time information available on Reddit. He realizes that the model lacks the updated context from Reddit discussions, leading to outdated responses based on the deprecated function. \n",
    "\n",
    "- Integration of Reddit Data: \n",
    "\n",
    "    - To bridge this gap, Bob decides to connect the model to Reddit using the Reddit API. \n",
    "\n",
    "    - He retrieves relevant data from the XYZNODE subreddit, which contains discussions pertinent to the node library. \n",
    "\n",
    "    - By extracting post titles, content, and comments, Bob ensures that the model gains access to the latest insights and solutions shared by users, including Alice's workaround. \n",
    "\n",
    "- Storing and Processing Reddit Data: \n",
    "\n",
    "    - Bob structures the retrieved Reddit data and stores it in a suitable format for analysis. \n",
    "\n",
    "    - He creates a new table in the company's vector storage, enabling efficient storage and retrieval of Reddit data along with embeddings for enhanced processing. \n",
    "\n",
    "- Utilizing Reddit Context in Model Queries: \n",
    "\n",
    "    - Bob modifies the existing functions, run_vector_search() and ask_llm(), to incorporate context from Reddit data and comments. \n",
    "\n",
    "    - He ensures that the model considers the updated context from Reddit discussions when generating responses to developer queries. \n",
    "\n",
    "- Testing and Validation: \n",
    "\n",
    "    - Bob conducts thorough testing to validate the integration of Reddit data into the model. \n",
    "\n",
    "    - He verifies that the model's responses now reflect the latest information and solutions discussed on Reddit, including Alice's workaround for the deprecated function. \n",
    "\n",
    "\n",
    "You can connect to reddit using their api to use their data for retrieval augmented generation. In this example, I am using the data from a subreddit called 'python' to do Retrieval Augmented Generation about python programming language.\n",
    "\n",
    "Connect to the reddit api using your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea9a450-2a44-4d1f-9803-9a0fed2a6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = \"RgqhQFaqxivVv2pBZ3jdYQ\"\n",
    "SECRET_KEY = \"pa7uK32LA-6ssF5Eaz7GDRxzsLurKw\"\n",
    "import requests\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID,SECRET_KEY)\n",
    "data = {\n",
    "    'grant_type':'password',\n",
    "    'username':'abhijithbabusap',\n",
    "    'password':'Workshop!234'\n",
    "}\n",
    "headers = {'User-Agent':'MyAPI/0.0.1'}\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',auth=auth,data=data,headers=headers)\n",
    "TOKEN = res.json()['access_token']\n",
    "headers['Authorization'] = f'bearer {TOKEN}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29f5fc-6524-4863-bd0d-106643f6fa82",
   "metadata": {},
   "source": [
    "Here we used our credentials to generate a token for the reddit API. The token will be stored in the headers, which will be passed along with every API call.\n",
    "\n",
    "Now we can access the posts on the subreddit 'saphanagraph'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92eb09f8-d41c-4a04-a8f6-10a727700bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = requests.get('https://oauth.reddit.com/r/saphanagraph',headers=headers).json()['data']['children']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37b9a8-67f4-4b80-a4e9-2be78b2fe376",
   "metadata": {},
   "source": [
    "Now run the below code snippet to extract the comments and replies of all the posts in the subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b847a56a-4494-485c-9077-94c23939ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [data['data']['id'] for data in reddit_data]\n",
    "comments = {}\n",
    "for i in ids:\n",
    "    comment_data = requests.get(f'https://oauth.reddit.com/r/saphanagraph/comments/{i}',headers=headers).json()\n",
    "    comments[f\"{i}\"] = []\n",
    "    def parse_comments(data):\n",
    "        for j in data:\n",
    "            comments[f\"{i}\"].append(j['data']['body'])\n",
    "            if j['data']['replies'] != '':\n",
    "                parse_comments(j['data']['replies']['data']['children'])\n",
    "    if(len(comment_data[1]['data']['children'])):\n",
    "        parse_comments(comment_data[1]['data']['children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3047a-073d-485c-a442-2a6b0459b6d3",
   "metadata": {},
   "source": [
    "Now generate the vector embeddings for the comments and create a pandas dataframe with the data extracted from the reddit, and their vector embeddings. Before sending the data to model we would anonymize the data and then Send the data if content has positive intention will save it as embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03394243-d854-4085-ba84-9c0d07b2c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def user_intention(query) -> str:\n",
    "\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "    response =  llm.invoke(\"Classify the following query in one word as negative or positive : \" + query).content\n",
    "    if \"negative\" in response.lower():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "def get_embedding(input, model=\"text-embedding-ada-002\") -> str:\n",
    "    response = embeddings.create(\n",
    "      model_name=model,\n",
    "      input=input\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3a54d-3c8a-4e65-be9f-d3d67008133f",
   "metadata": {},
   "source": [
    "Here we defined a user_intention() function that classifies data from reddit into positive and negative data. \n",
    "\n",
    "Anonymizing content and removing content that has negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71f59e3a-5201-4917-9e44-70ecd3c39dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: presidio-anonymizer in c:\\users\\slcib\\anaconda3\\lib\\site-packages (2.2.354)\n",
      "Requirement already satisfied: pycryptodome>=3.10.1 in c:\\users\\slcib\\anaconda3\\lib\\site-packages (from presidio-anonymizer) (3.20.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slcib\\anaconda3\\lib\\site-packages\\torch\\_masked\\__init__.py:223: UserWarning: Failed to initialize NumPy: module compiled against API version 0xf but this version of numpy is 0xe (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  example_input = torch.tensor([[-3, -2, -1], [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "!pip install presidio-anonymizer\n",
    "import pandas as pd\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\n",
    "\n",
    "analyzer = AnalyzerEngine(log_decision_process=False)\n",
    "batch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\n",
    "\n",
    "extracted_data = []\n",
    "for post in enumerate(reddit_data):\n",
    "    post_content = post[1]['data']['selftext']\n",
    "    post_content = reddit_data[0]['data']['selftext']\n",
    "    anonymizer_results = analyzer.analyze(text=post_content, entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"IP_ADDRESS\"], language=\"en\", return_decision_process=True)\n",
    "    for i in anonymizer_results:\n",
    "        post_content = post_content.replace(i.entity_value, i.anonymized_value)\n",
    "    if user_intention(post_content):\n",
    "        row = [post[0]+1]\n",
    "        row.append(post[1]['data']['title'])\n",
    "        row.append(post_content)\n",
    "        row.append(str(comments[post[1]['data']['id']]))\n",
    "        row.append(str(get_embedding(row[1])))\n",
    "        extracted_data.append(row)\n",
    "# df = pd.DataFrame(extracted_data)\n",
    "# df.columns = [\"ID\", \"Title\", \"Post\", \"Comments\", \"Post-Vector\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ecc60-8d0e-4688-a24b-6a35ae487b82",
   "metadata": {},
   "source": [
    "You might have to restart your notebook after running the above code. Once you restart the notebook, start again from connecting to the reddit API.\n",
    "\n",
    "In the above code, we extracted the data from reddit and stored it in a dataframe. We used a batch_analyzer and batch_anonymizer to remove sensitive data from the data coming from reddit.\n",
    "\n",
    "\n",
    "Upload the data to your HANA vector storage as we saw in the previous section.\n",
    "\n",
    "Add your credentials to create your connectionContext\n",
    "\n",
    "Replace TABLENAME with a new table that you want to create. **Make sure that you use only uppercase letters and numbers in the tablename.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db56769c-415b-462a-8056-4a1712d1044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml import ConnectionContext\n",
    "from hana_ml.dataframe import create_dataframe_from_pandas\n",
    "\n",
    "cc= ConnectionContext(\n",
    "    address='4e77f46b-5af4-4cdf-96b6-2c7cae6ddb39.hana.prod-eu10.hanacloud.ondemand.com',\n",
    "    port='443',\n",
    "    user='DBADMIN',\n",
    "    password='Initial1',\n",
    "    encrypt=True\n",
    "    )\n",
    "\n",
    "\n",
    "cursor = cc.connection.cursor()\n",
    "sql_command = '''CREATE TABLE TEST08(ID BIGINT, TITLE NCLOB, TEXT NCLOB, COMMENT NCLOB, VECTOR_STR REAL_VECTOR);'''\n",
    "cursor.execute(sql_command)\n",
    "cursor.close()\n",
    "\n",
    "cursor = cc.connection.cursor()\n",
    "sql_insert = 'INSERT INTO TEST08(ID, TITLE, TEXT, COMMENT, VECTOR_STR) VALUES (?,?,?,?,TO_REAL_VECTOR(?))'\n",
    "cursor.executemany(sql_insert,extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f75bb-3e48-4973-ac2b-1290787d23d2",
   "metadata": {},
   "source": [
    "\n",
    "The embeddings can be used to find texts that are similar to each other using techniques such as COSINE-SIMILARITY. You can use such techniques to find text data similar to your prompt from the HANA vector store.\n",
    "\n",
    "Now define a function to do similarity search in your data. This function takes a query as input and returns similar content from the table.\n",
    "\n",
    "Replace the TABLENAME with the name of your table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d754c658-8603-425c-9abd-47dd862b5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vector_search(query: str, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    if metric == 'L2DISTANCE':\n",
    "        sort = 'ASC'\n",
    "    else:\n",
    "        sort = 'DESC'\n",
    "    query_vector = get_embedding(query)\n",
    "    sql = '''SELECT TOP {k} \"TITLE\", \"TEXT\", \"COMMENT\"\n",
    "        FROM \"TEST08\"\n",
    "        ORDER BY \"{metric}\"(\"VECTOR_STR\", TO_REAL_VECTOR('{qv}')) {sort}'''.format(k=k, metric=metric, qv=query_vector, sort=sort)\n",
    "    hdf = cc.sql(sql)\n",
    "    df_context = hdf.head(k).collect()\n",
    "    return df_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aea3a9-1283-4942-b4fa-8908f8f12fde",
   "metadata": {},
   "source": [
    "We can define a prompt template to provide context to our prompts. Thus, when we pass a prompt to the model, the template will add the necessary context to the prompt so that better results are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f80a5e94-c037-4704-acd4-fd92e102ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate_fstring = \"\"\"\n",
    "You are an Python expert.\n",
    "You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "promptTemplate = PromptTemplate.from_template(promptTemplate_fstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae837ab-c194-433d-be47-32eac49497e7",
   "metadata": {},
   "source": [
    "Here we have created a template for the prompts. It contains two variables, 'context' and 'query'. These variables will be replaced with the context and query in the upcoming steps \n",
    "\n",
    "Now we can define a function ask_llm() that queries a model using the above template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68b5a612-4bb4-45f8-9003-540e76cdcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def ask_llm(query: str, metric='COSINE_SIMILARITY', k = 4) -> str:\n",
    "    context = ''\n",
    "    context = run_vector_search(query, metric, k)\n",
    "    prompt = promptTemplate.format(query=query, context=' '.join(context['TEXT'].astype('string')))\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "    response = llm.invoke(prompt).content\n",
    "    print('Query: '+ query)\n",
    "    print('\\nResponse:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c00362-c55c-4816-ab42-d453062d41df",
   "metadata": {},
   "source": [
    "The above function appends the query and context into the template to create a prompt. Then that prompt is passed on to the LLM model and the response is printed.\n",
    "\n",
    "Here we are using gpt-4 model. Make sure that you have already created a deployment for the gpt-4 model in AI Launchpad\n",
    "\n",
    "Now we can test the function by passing the following query to generate results using retrieval augmented generation.ut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f212e250-846e-4640-8c59-749803235fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How can I search for a word in a dictionary?\n",
      "\n",
      "Response:\n",
      "In Python, you can search for a word in a dictionary using the 'in' keyword. The 'in' keyword checks if a word (or key) is present in the dictionary. Here's an example:\n",
      "\n",
      "```python\n",
      "# Define a dictionary\n",
      "my_dict = {'apple': 1, 'banana': 2, 'orange': 3}\n",
      "\n",
      "# Word to search\n",
      "word = 'banana'\n",
      "\n",
      "# Check if word is in dictionary\n",
      "if word in my_dict:\n",
      "    print(f\"{word} is in the dictionary.\")\n",
      "else:\n",
      "    print(f\"{word} is not in the dictionary.\")\n",
      "```\n",
      "\n",
      "In this code, 'banana' is a key in the dictionary. The 'in' keyword checks if 'banana' is among the dictionary's keys and prints a message based on whether it is found or not.\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I search for a word in a dictionary?\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac31b22-6fca-4aa0-9378-148cfc809c3a",
   "metadata": {},
   "source": [
    "We got a detailed output in contrast to the output for the same query without RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e273ba-5894-42d9-aab5-8fbc3f555048",
   "metadata": {},
   "source": [
    "We can see that the output generated is based on the comment that we posted in Reddit.\n",
    "\n",
    "**Outcome:** Through the integration of live Reddit data, Bob enhances the model's ability to provide up-to-date and relevant support to developers. By leveraging real-time insights and solutions shared by the community, the help portal powered by embeddings becomes even more valuable, addressing developers' queries with greater accuracy and efficiency.\n",
    "\n",
    "### Code generation\n",
    "\n",
    "We can make use of the LLM to generate code in python and other programming languages. In the following example, we make the model write a code to search for a word in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42fa676f-5025-4747-a981-6c27050c15f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple Python code that can be used to search for a word in a dictionary.\n",
      "\n",
      "```python\n",
      "# Define the dictionary\n",
      "my_dict = {\n",
      "    'apple': 'fruit',\n",
      "    'carrot': 'vegetable',\n",
      "    'dog': 'animal',\n",
      "    'elephant': 'animal',\n",
      "    'flower': 'nature'\n",
      "}\n",
      "\n",
      "# Function to search for a word in the dictionary\n",
      "def search_dict(word):\n",
      "    if word in my_dict:\n",
      "        print(f\"'{word}' found in the dictionary with value '{my_dict[word]}'\")\n",
      "    else:\n",
      "        print(f\"'{word}' not found in the dictionary\")\n",
      "\n",
      "# Test the function\n",
      "search_dict('apple')\n",
      "search_dict('cat')\n",
      "```\n",
      "\n",
      "In this code, we define a dictionary `my_dict` and a function `search_dict(word)`. The function checks if the word is present in the dictionary using the `in` keyword. If the word is found, it prints the word and its corresponding value. If the word is not found, it prints a message saying the word is not found.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "response = llm.invoke(\"write the python code to search for a word in a dictionary?\").content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc5d2c-893c-4b19-89bd-8a13e5d5a8c6",
   "metadata": {},
   "source": [
    "We can paste the generated code into a new cell here and run it to see its output\n",
    "\n",
    "Now we can use RAG to generate code. In the following example, we make use of RAG to generate code using the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8620247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'banana' found in the dictionary.\n"
     ]
    }
   ],
   "source": [
    "# Define the dictionary\n",
    "dictionary = {\"apple\": \"A fruit\", \"banana\": \"A yellow fruit\", \"cherry\": \"A small, round stone fruit\"}\n",
    "\n",
    "# Define the word to search\n",
    "word_to_search = \"banana\"\n",
    "\n",
    "# Check if the word is in the dictionary\n",
    "if word_to_search in dictionary:\n",
    "    print(f\"'{word_to_search}' found in the dictionary.\")\n",
    "else:\n",
    "    print(f\"'{word_to_search}' not found in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caa8799e-f948-4bdc-82d9-b224a328be47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: write the python code to search for a word in a dictionary?\n",
      "\n",
      "Response:\n",
      "Sure, here is a simple Python code snippet to search for a word within a dictionary:\n",
      "\n",
      "```python\n",
      "def search_in_dictionary(word, dictionary):\n",
      "    if word in dictionary:\n",
      "        print(f\"'{word}' found in dictionary.\")\n",
      "    else:\n",
      "        print(f\"'{word}' not found in dictionary.\")\n",
      "\n",
      "my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
      "\n",
      "search_in_dictionary('key2', my_dict)\n",
      "```\n",
      "\n",
      "This code defines a function `search_in_dictionary` that takes a word and a dictionary as input. It checks if the word is in the dictionary keys, and prints a message indicating whether the word was found. The function is then tested with a sample dictionary `my_dict` and a search word 'key2'.\n"
     ]
    }
   ],
   "source": [
    "query = \"write the python code to search for a word in a dictionary?\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5458ec-f19d-459d-9dbb-3769c0ef14a4",
   "metadata": {},
   "source": [
    "We can paste the generated code into a new cell and run it to see its output.\n",
    "\n",
    "We can make use of the LLM to find the difference between the codes generated in both cases. Copy paste the code into the following prompt and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59a165dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'banana' found in the dictionary with the meaning 'a yellow fruit'.\n"
     ]
    }
   ],
   "source": [
    "my_dict = {\"apple\": \"a fruit\", \"banana\": \"a yellow fruit\", \"cherry\": \"a small, round, red fruit\"}\n",
    "\n",
    "# The word to search for\n",
    "word_to_find = \"banana\"\n",
    "\n",
    "if word_to_find in my_dict:\n",
    "    print(f\"'{word_to_find}' found in the dictionary with the meaning '{my_dict[word_to_find]}'.\")\n",
    "else:\n",
    "    print(f\"'{word_to_find}' not found in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6413733e-d710-48eb-9198-688c17e634a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences found:\n",
      "Line 2:\n",
      "  Code-1: dictionary = {\"apple\": \"A fruit\", \"banana\": \"A yellow fruit\", \"cherry\": \"A small, round stone fruit\"}\n",
      "  Code-2: my_dict = {\"apple\": \"a fruit\", \"banana\": \"a yellow fruit\", \"cherry\": \"a small, round, red fruit\"}\n",
      "  Explanation: Variable names differ (dictionary vs my_dict).\n",
      "Line 3:\n",
      "  Code-1: word_to_search = \"banana\"\n",
      "  Code-2: word_to_find = \"banana\"\n",
      "Line 5:\n",
      "  Code-1: if word_to_search in dictionary:\n",
      "  Code-2: if word_to_find in my_dict:\n",
      "Line 6:\n",
      "  Code-1:     print(f\"'{word_to_search}' found in the dictionary.\")\n",
      "  Code-2:     print(f\"'{word_to_find}' found in the dictionary with the meaning '{my_dict[word_to_find]}'.\")\n",
      "  Explanation: Variable names differ (word_to_search vs word_to_find).\n",
      "Line 8:\n",
      "  Code-1:     print(f\"'{word_to_search}' not found in the dictionary.\")\n",
      "  Code-2:     print(f\"'{word_to_find}' not found in the dictionary.\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compare_and_explain(code1, code2):\n",
    "    # Split the code into lines\n",
    "    lines1 = code1.strip().split('\\n')\n",
    "    lines2 = code2.strip().split('\\n')\n",
    "    \n",
    "    # Find differences in lines\n",
    "    diff_lines = []\n",
    "    for i, (line1, line2) in enumerate(zip(lines1, lines2), 1):\n",
    "        if line1 != line2:\n",
    "            diff_lines.append((i, line1, line2))\n",
    "    \n",
    "    if not diff_lines:\n",
    "        return \"Both code snippets are identical.\"\n",
    "    \n",
    "    # Generate difference report with explanations\n",
    "    report = \"Differences found:\\n\"\n",
    "    for line_num, line1, line2 in diff_lines:\n",
    "        report += f\"Line {line_num}:\\n\"\n",
    "        report += f\"  Code-1: {line1}\\n\"\n",
    "        report += f\"  Code-2: {line2}\\n\"\n",
    "        \n",
    "        # Explanation for the differences\n",
    "        if line_num == 2:  # Line with the dictionary definition\n",
    "            report += \"  Explanation: Variable names differ (dictionary vs my_dict).\\n\"\n",
    "        elif line_num == 6:  # Line with the word to search\n",
    "            report += \"  Explanation: Variable names differ (word_to_search vs word_to_find).\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Define the two code snippets to compare\n",
    "code1 = '''\n",
    "query = \"\"\n",
    "dictionary = {\"apple\": \"A fruit\", \"banana\": \"A yellow fruit\", \"cherry\": \"A small, round stone fruit\"}\n",
    "word_to_search = \"banana\"\n",
    "\n",
    "if word_to_search in dictionary:\n",
    "    print(f\"'{word_to_search}' found in the dictionary.\")\n",
    "else:\n",
    "    print(f\"'{word_to_search}' not found in the dictionary.\")\n",
    "'''\n",
    "\n",
    "code2 = '''\n",
    "query = \"\"\n",
    "my_dict = {\"apple\": \"a fruit\", \"banana\": \"a yellow fruit\", \"cherry\": \"a small, round, red fruit\"}\n",
    "word_to_find = \"banana\"\n",
    "\n",
    "if word_to_find in my_dict:\n",
    "    print(f\"'{word_to_find}' found in the dictionary with the meaning '{my_dict[word_to_find]}'.\")\n",
    "else:\n",
    "    print(f\"'{word_to_find}' not found in the dictionary.\")\n",
    "'''\n",
    "\n",
    "# Compare the two code snippets\n",
    "result = compare_and_explain(code1, code2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a313699",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt-4-turbo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31459653-642d-41df-88b6-4612ae010b43",
   "metadata": {},
   "source": [
    "Here we can see how the codes differ from each other.\n",
    "\n",
    "**Outcome:** After generating the code, Bob tests the code and pushes it to github for testing and quality assurance. The effort to write code is reduced by a big margin by making use of the LLMs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
