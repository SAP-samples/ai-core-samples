{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3defa59-142d-4906-add5-b809d01d126b",
   "metadata": {},
   "source": [
    "## Utilizing RAG Embeddings for Enhanced Developer Support\n",
    "\n",
    "Bob is a Content  developer in the XYZ company. His team works on a node library that can simplify the software development process inside and outside the company. His team has designed a dedicated documentation for the library. But the developers who use the library always face issues and contact Bob. To make his job easy, Bob decided to use Generative AI for this Use case. \n",
    "\n",
    "Bob uses the Generative AI Hub SDK to create a help portal for his library. He wants to Improve the effectiveness of our help portal by integrating RAG (Retrieval-Augmented Generation) embeddings so that Developers can receive more contextually relevant responses to their queries about the node library. \n",
    "\n",
    "Acceptance criteria: \n",
    "\n",
    "- Bob utilizes the Generative AI Hub SDK to integrate a generative AI model into the help portal for the company's node library. \n",
    "\n",
    "- Bob configures and deploys the generative AI model within the AI core instance of the company's infrastructure. \n",
    "\n",
    "- The help portal powered by Generative AI should provide intuitive and contextually relevant responses to developers' inquiries about using the node library. \n",
    "\n",
    "- The generative AI model is trained on the documentation of the node library to ensure accurate and informative responses. \n",
    "\n",
    "- The help portal interface is user-friendly, allowing developers to easily interact with the generative AI model and find answers to their questions. \n",
    "\n",
    "- Bob monitors the performance of the generative AI model regularly, ensuring that it continues to provide accurate and helpful responses to developers' queries. \n",
    "\n",
    "- Bob documents the process of integrating the Generative AI Help Portal into the company's support workflow, providing clear instructions for future maintenance and updates. \n",
    "\n",
    "- Bob Queries the model \n",
    "\n",
    "- Implements RAG embeddings \n",
    "\n",
    "- Enhances Query responses \n",
    "\n",
    "- Bob conducts training sessions or provides documentation to the support team, educating them on how to leverage the Generative AI Help Portal effectively to assist developers. \n",
    "\n",
    "### Setting up LLM's on AI core\n",
    "\n",
    "- Bob navigates to ML Operations -> Scenarios in the AI-Core workspace. \n",
    "\n",
    "- He ensures that the foundation-models scenario is present. \n",
    "\n",
    "- Bob goes to ML Operations -> Configurations and creates a new configuration: \n",
    "\n",
    "- Names it appropriately. \n",
    "\n",
    "- Selects the foundation-models scenario. \n",
    "\n",
    "- Chooses the version and executable ID. \n",
    "\n",
    "- He sets input parameters specifying the model name and version. \n",
    "\n",
    "- Bob reviews and creates the configuration. \n",
    "\n",
    "Model Deployment: \n",
    "\n",
    "- Bob creates a deployment for the configured model, setting the duration to standard. \n",
    "\n",
    "- After the deployment status changes to RUNNING, he verifies its availability. \n",
    "\n",
    "Model Integration: \n",
    "\n",
    "- Bob installs the Generative AI Hub SDK by running pip3 install generative-ai-hub-sdk. \n",
    "\n",
    "- He configures the SDK using AI core keys stored in the local directory. \n",
    "\n",
    "\n",
    "\n",
    "- Bob uses Python code to generate responses for queries using the SDK. \n",
    "\n",
    "- He formats the query and invokes the Generative AI Hub SDK to fetch responses. \n",
    "\n",
    "Implementing RAG Embeddings: \n",
    "\n",
    "- Bob prepares the documentation for the node library in CSV format with each row representing a topic. \n",
    "\n",
    "- He connects to the HANA vector storage instance and creates a table to store the documentation data. \n",
    "\n",
    "- Bob populates the table with data and creates a REAL_VECTOR column to store embeddings. \n",
    "\n",
    "- Using the Generative AI Hub SDK, he defines a function to generate embeddings for prompts and performs similarity search using the embeddings. \n",
    "\n",
    "Enhancing Query Responses: \n",
    "\n",
    "- Bob defines a prompt template to provide context to queries. \n",
    "\n",
    "- He modifies the function to query the LLM (Large Language Model) based on the prompt template. \n",
    "\n",
    "- Bob tests the model's response using specific queries related to the node library, ensuring it provides contextually relevant responses based on embeddings.  Bob installs Jupyter Notebook using pip3 install notebook. \n",
    "\n",
    "Retrieval augmented generation optimizes the output of large language models by giving more contexts to your prompts.\n",
    "\n",
    "In this workshop, we use a [graph document](files/GRAPH_DOCU_2503.csv) to give more context to the model, thus optimiz\n",
    "iLoad the document into the data variable. is a structure that contains two-dimensional data and its corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install langchain openai docarray panel jupyter_bokeh redlines tiktoken wikipedia DateTime presidio_anonymizer hana-ml pandas presidio_analyzer\n",
    "!pip3 install generative-ai-hub-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d388df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An alternative way to config.json to set the environment variables for gen-ai-hub-sdk\n",
    "\n",
    "import os\n",
    "\n",
    "# please enter the Credentials from your AI core landscape.\n",
    "env_vars = {\n",
    "    'AICORE_AUTH_URL' : '<auth_url>',\n",
    "    'AICORE_CLIENT_ID' : '<client_id>',\n",
    "    'AICORE_CLIENT_SECRET' : '<client_secret>',\n",
    "    'AICORE_BASE_URL' : '<api_url>/v2',\n",
    "    'AICORE_RESOURCE_GROUP' : '<resource_group>',\n",
    "}\n",
    "\n",
    "# Set the environment variables using `os.environ`.\n",
    "for key, value in env_vars.items():\n",
    "    os.environ[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2456b27",
   "metadata": {},
   "source": [
    "The data in GRAPH_DOCU_QRC3.csv already contains vector embeddings. The only reason why we included the vectors in the csv is to enable colleagues to try out vector search very fastâ€¦ no need to run the content through an embedding function.\n",
    "\n",
    "This dataset is created from the SAP HANA Cloud documentation, in specific the Graph Engine part (https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-graph-reference/sap-hana-cloud-sap-hana-database-graph-reference).\n",
    "SAP Help Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59c835-d853-429a-bbbf-78d1c9894586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('GRAPH_DOCU_2503.csv', encoding='utf-8') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            data.append(row)\n",
    "        except:\n",
    "            print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe7f05-ae81-46b2-a9bd-ec4eddc31f6a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now we can create a connection to the HANA Vector storage. Replace the address, port-number, username, and password from the values in your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f80d2-29e4-4fc1-8b59-1304425bf2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbcli\n",
    "from hdbcli import dbapi\n",
    "\n",
    "cc = dbapi.connect(\n",
    "    address='<host>',\n",
    "    port='443',\n",
    "    user='<user>',\n",
    "    password='<password>',\n",
    "    encrypt=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8ea5e-0ae7-442e-8f69-97e22cee88bf",
   "metadata": {},
   "source": [
    "Now we have created a connection to the HANA vector storage. This connection will help us to work with the vector storage in the upcoming steps.\n",
    "\n",
    "HANA vector storage stores data in form of tables. To store our data, we can create a table in our HANA vector storage.\n",
    "\n",
    "Replace the TABLENAME with a name of your choice. **Make sure that you use only Uppercase letters and numbers in the tablename**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55e535a-1997-42f9-a56b-822d0f6d3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table\n",
    "cursor = cc.cursor()\n",
    "sql_command = '''CREATE TABLE TABLENAME(ID1 BIGINT, ID2 BIGINT, L1 NVARCHAR(3), L2 NVARCHAR(3), L3 NVARCHAR(3), FILENAME NVARCHAR(100), HEADER1 NVARCHAR(5000), HEADER2 NVARCHAR(5000), TEXT NCLOB, VECTOR_STR REAL_VECTOR);'''\n",
    "cursor.execute(sql_command)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1968b049-c95e-4d3d-92f2-df853e66fce9",
   "metadata": {},
   "source": [
    "In the SQL command, we are creating a table called 'GRAPH_DOCU_2503'. You can give any name of your choice, but make sure that you use the same table name throughout. Also keep in mind that you cannot create multiple tables with the same name.\n",
    "\n",
    "Once you have created the table, you can populate the table with our data which we have converted to the dataframe in the previous step. Here the SQL insert is inserting everything from the CSV file that includes Text chunks, Embeddings and metadata. this is to show how you can use can insert the text chunks to hana vector DB for RAG to be implemented.\n",
    "\n",
    "Replace the TABLENAME with the name of your table. this for insertion of embeddings that are pre-existing in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641461c5-1c5d-45b5-999a-805816b3ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cc.cursor()\n",
    "sql_insert = 'INSERT INTO TABLENAME(ID1, ID2, L1, L2, L3, FILENAME, HEADER1, HEADER2, TEXT, VECTOR_STR) VALUES (?,?,?,?,?,?,?,?,?,TO_REAL_VECTOR(?))'\n",
    "cursor.executemany(sql_insert,data[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7efc76b-5542-425e-a722-567f2ce12d63",
   "metadata": {},
   "source": [
    "\n",
    "Embeddings are vector representations of text data that incorporates the semantic meaning of the text. Define a get_embedding() function that generates embeddings from text data using the text-embedding-ada-002 model. This function will be used to generate embeddings from the user's prompts.\n",
    "\n",
    "For example if user enters the prompt for \"How can I run a shortest path algorithm?\" the below get_embeddings function would be used to convert the user prompt to embedding which would be used to search similar data in Hana DB using similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d1974-c692-4b30-8a2a-a044f71fa654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "def get_embedding(input, model=\"text-embedding-ada-002\") -> str:\n",
    "    response = embeddings.create(\n",
    "      model_name=model,\n",
    "      input=input\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853dcfd7-ffa9-4d46-a66e-5a673473174d",
   "metadata": {},
   "source": [
    "\n",
    "The embeddings can be used to find texts that are similar to each other using techniques such as COSINE-SIMILARITY. You can use such techniques to find text data similar to your prompt from the HANA vector store.\n",
    "\n",
    "Now define a function to do similarity search in your data. This function takes a query as input and returns similar content from the table.\n",
    "\n",
    "Replace the TABLENAME with the name of your table.\n",
    "\n",
    "Here in the below code the code we are trying to find the similar data to users query where it is calling get_embedding functions and storing it in query_vector and with a simple select query to Hana DB we can find the 4 similar embeddings data using COSINE_SIMILARITY as search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548537af-f2cb-4955-ba39-4d9407ad9acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cc.cursor()\n",
    "def run_vector_search(query: str, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    if metric == 'L2DISTANCE':\n",
    "        sort = 'ASC'\n",
    "    else:\n",
    "        sort = 'DESC'\n",
    "    query_vector = get_embedding(query)\n",
    "    sql = '''SELECT TOP {k} \"ID2\", \"TEXT\"\n",
    "        FROM \"TABLENAME\"\n",
    "        ORDER BY \"{metric}\"(\"VECTOR_STR\", TO_REAL_VECTOR('{qv}')) {sort}'''.format(k=k, metric=metric, qv=query_vector, sort=sort)\n",
    "    cursor.execute(sql)\n",
    "    hdf = cursor.fetchall()\n",
    "    return hdf[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94990eca-90d2-4aa2-8833-a3187c47aeba",
   "metadata": {},
   "source": [
    "We can define a prompt template to provide context to our prompts. Thus, when we pass a prompt to the model, the template will add the necessary context to the prompt so that better results are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39bd7c-0234-4b3f-9bd3-0329524ed18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate_fstring = \"\"\"\n",
    "You are an SAP HANA Cloud expert.\n",
    "You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{query}\n",
    "\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "promptTemplate = PromptTemplate.from_template(promptTemplate_fstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbd6bd2-1443-430c-8248-febc27512bdd",
   "metadata": {},
   "source": [
    "Here we have created a template for the prompts. It contains two variables, 'context' and 'query'. These variables will be replaced with the context and query in the upcoming steps \n",
    "\n",
    "Now we can define a function ask_llm() that queries a model using the above template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c7f4b-a42f-462d-a2bb-a77f8ba41bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def ask_llm(query: str, metric='COSINE_SIMILARITY', k = 4) -> str:\n",
    "    context = ''\n",
    "    context = run_vector_search(query, metric, k)\n",
    "    prompt = promptTemplate.format(query=query, context=' '.join(str(context)))\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(str(prompt)))\n",
    "    print('no of tokens'+ str(num_tokens))\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k',max_tokens = 8000)\n",
    "    response = llm.invoke(prompt).content\n",
    "    print('Query: '+ query)\n",
    "    print('\\nResponse:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fa8a66-8241-4e81-a7d9-c632239edbdf",
   "metadata": {},
   "source": [
    "The above function appends the query and context into the template to create a prompt. Then that prompt is passed on to the LLM model and the response is printed.\n",
    "\n",
    "Here we are using gpt-4 model. Make sure that you have already created a deployment for the gpt-4 model in AI Launchpad.\n",
    "\n",
    "We can compare how the output produced by RAG is different from the output when we directly pass the prompt to the model. If we directly pass the prompt to the model withouth RAG, this will be the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617f819-e8fd-42ed-b5c1-b3679d3c187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I run a shortest path algorithm?\"\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "response = llm.invoke(query).content\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf4ffbe-e081-4f7c-80b8-5a5e2f54da30",
   "metadata": {},
   "source": [
    "Now we can test the function by passing the following query to generate results using retrieval augmented generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61ff1a-6d15-4f2e-a17a-eb51d02c6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"I want to calculate a shortest path. How do I do that?\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38bdada-6fb3-45dd-b314-4027b1beef71",
   "metadata": {},
   "source": [
    "Here we can see that we get better output when we use RAG\n",
    "\n",
    "**Outcome:** By implementing RAG embeddings in the help portal, Bob enhances the support experience for developers using the node library. The help portal now offers more accurate and contextual responses to queries, reducing dependency on manual support and empowering developers to find solutions independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3870db-1e0d-45f9-9e28-0fb51d9f5698",
   "metadata": {},
   "source": [
    "## Prompting the model\n",
    "\n",
    "The LLM can do a wide variety of tasks using prompting techniques. By efficient use of prompts, the models can achieve good results in the following tasks\n",
    "\n",
    "**Content creation using RAG**\n",
    "\n",
    "The model can generate text content for articles, blogs, stories, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31aec6-e78f-45d1-be84-1369686e9887",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Write an article on the new features in graphs.\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357784ec-0951-4efc-9d85-22986ec5b43e",
   "metadata": {},
   "source": [
    "\n",
    "**Language Translation using RAG**\n",
    "\n",
    "If you want to get the output in another language, you can ask the model to do so in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86afecfb-32d4-48d3-a40f-d8f1f35d53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I find the shortest distance between two nodes? Give the output in Dutch.\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42747555-60a1-45de-94fd-aaa29f729add",
   "metadata": {},
   "source": [
    "**Text Completion using RAG**\n",
    "\n",
    "The models can be used for text completions. By giving a text input, the models suggests completions for the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf016cb7-8bde-4ff2-a5c3-c36978175cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Graphs can be a useful tool for computer scientists when ...\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f78e44-1ab4-4ba6-8787-e3a632de4c16",
   "metadata": {},
   "source": [
    "**Text Classification using RAG**\n",
    "\n",
    "The models can be used to classify text based on their semantics. In the following example, the model can classify whether a question is related to graphs or related to business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc730b-59a0-4f7b-9939-887e60d47ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''Classify the following questions into graph related and business related questions. \n",
    "\n",
    "    1. How does diversification reduce investment risk?\n",
    "    2. Describe the role of dividends in stock investing.\n",
    "    3. What is the degree of a node in a graph, and how is it calculated?\n",
    "    4. What factors influence consumer behavior in marketing?\n",
    "    5. Explain the basic principles of the law of diminishing returns.\n",
    "    6. What is the concept of opportunity cost in economics?\n",
    "    7. What is the definition of a graph in graph theory?\n",
    "    8. How are directed and undirected graphs distinguished from each other?\n",
    "    9. Explain the difference between a path and a cycle in a graph.\n",
    "    10. What is a connected graph, and why is it significant in the study of graphs?'''\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867db6b-8679-4d62-bdae-8310191dcf58",
   "metadata": {},
   "source": [
    "\n",
    "**Tone adjustment using RAG**\n",
    "\n",
    "The models can adjust the tone of a content according to the instructions given. In the following example, we see a snippet from an article that has an anger tone, and we ask the LLM to make it professional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456cfae-2559-4da2-afd0-488511491c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    The following conent is not fit for a professional document. Rewrite the same in a professional tone. \n",
    "        \n",
    "    I can't believe we're still dealing with this nonsense! If you dare to mess around with the referenced graph tables without ensuring the consistency of the graph workspace through referential constraints, you're practically asking for chaos! And let me tell you, when that happens, each blasted Graph Engine component behaves differently! It's like dealing with a bunch of unruly children who can't agree on a single thing!\n",
    "\n",
    "    Check out the CREATE GRAPH WORKSPACE Statement and the DROP GRAPH WORKSPACE Statement. Maybe if you bothered to read them properly, you wouldn't be stuck in this mess! But no, here we are, dealing with your incompetence and the fallout of your reckless actions. Get your act together and start taking responsibility for maintaining a freaking consistent graph workspace!'''\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1709b16-1ec7-44a5-9b47-0fe787f0a607",
   "metadata": {},
   "source": [
    "\n",
    "**Spell Check / Grammar Check using RAG**\n",
    "\n",
    "The model can also detect the spelling and grammatical issues in a document and correct them promptly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e691c9b-7c20-45a7-82d0-99f4a7079a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    Proofread and rewrite the following sentence:\n",
    "    The GraphScript langaguae  will eases the developomont of application-specific graph algthrihms and integrates them into SQL-based data procezzing.\n",
    "        '''\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00538abf-712f-46f4-a6ad-dc00a8c3412c",
   "metadata": {},
   "source": [
    "## Using data from reddit for Retrieval Augmented Generation\n",
    "\n",
    "As a Content Developer at XYZ Company, Bob's journey to improve developer support with embeddings encounters a new challenge when Alice, a colleague in his team, discovers critical information on Reddit. Here's how it integrates into the user story: \n",
    "\n",
    "- Discovery of Critical Information: \n",
    "\n",
    "    - Alice notices a Reddit post highlighting issues with a deprecated function in the node library. Recognizing the potential impact on developers, she investigates and finds a workaround, which she promptly shares as a reply to the post. \n",
    "\n",
    "- Increased Support Queries: \n",
    "     - The following day, Bob's team receives numerous inquiries related to the same issue discussed on Reddit. Despite Alice's timely response on the platform, developers continue to face challenges, leading to an influx of support queries. \n",
    "\n",
    "- Addressing Model Limitations: \n",
    "\n",
    "    - Bob identifies a gap between the support provided by their existing model and the real-time information available on Reddit. He realizes that the model lacks the updated context from Reddit discussions, leading to outdated responses based on the deprecated function. \n",
    "\n",
    "- Integration of Reddit Data: \n",
    "\n",
    "    - To bridge this gap, Bob decides to connect the model to Reddit using the Reddit API. \n",
    "\n",
    "    - He retrieves relevant data from the XYZNODE subreddit, which contains discussions pertinent to the node library. \n",
    "\n",
    "    - By extracting post titles, content, and comments, Bob ensures that the model gains access to the latest insights and solutions shared by users, including Alice's workaround. \n",
    "\n",
    "- Storing and Processing Reddit Data: \n",
    "\n",
    "    - Bob structures the retrieved Reddit data and stores it in a suitable format for analysis. \n",
    "\n",
    "    - He creates a new table in the company's vector storage, enabling efficient storage and retrieval of Reddit data along with embeddings for enhanced processing. \n",
    "\n",
    "- Utilizing Reddit Context in Model Queries: \n",
    "\n",
    "    - Bob modifies the existing functions, run_vector_search() and ask_llm(), to incorporate context from Reddit data and comments. \n",
    "\n",
    "    - He ensures that the model considers the updated context from Reddit discussions when generating responses to developer queries. \n",
    "\n",
    "- Testing and Validation: \n",
    "\n",
    "    - Bob conducts thorough testing to validate the integration of Reddit data into the model. \n",
    "\n",
    "    - He verifies that the model's responses now reflect the latest information and solutions discussed on Reddit, including Alice's workaround for the deprecated function. \n",
    "\n",
    "\n",
    "You can connect to reddit using their api to use their data for retrieval augmented generation. In this example, I am using the data from a subreddit called 'python' to do Retrieval Augmented Generation about python programming language.\n",
    "\n",
    "Connect to the reddit api using your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea9a450-2a44-4d1f-9803-9a0fed2a6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = \"<client_id>\"\n",
    "SECRET_KEY = \"<secret_key>\"\n",
    "import requests\n",
    "auth = requests.auth.HTTPBasicAuth(CLIENT_ID,SECRET_KEY)\n",
    "data = {\n",
    "    'grant_type':'password',\n",
    "    'username':'<username>',\n",
    "    'password':'<password>'\n",
    "}\n",
    "headers = {'User-Agent':'MyAPI/0.0.1'}\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',auth=auth,data=data,headers=headers)\n",
    "TOKEN = res.json()['access_token']\n",
    "headers['Authorization'] = f'bearer {TOKEN}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a29f5fc-6524-4863-bd0d-106643f6fa82",
   "metadata": {},
   "source": [
    "Here we used our credentials to generate a token for the reddit API. The token will be stored in the headers, which will be passed along with every API call.\n",
    "\n",
    "Now we can access the posts on the subreddit 'saphanagraph'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb09f8-d41c-4a04-a8f6-10a727700bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = requests.get('https://oauth.reddit.com/r/saphanagraph',headers=headers).json()['data']['children']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37b9a8-67f4-4b80-a4e9-2be78b2fe376",
   "metadata": {},
   "source": [
    "Now run the below code snippet to extract the comments and replies of all the posts in the subreddit. in comments variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847a56a-4494-485c-9077-94c23939ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [data['data']['id'] for data in reddit_data]\n",
    "comments = {}\n",
    "for i in ids:\n",
    "    comment_data = requests.get(f'https://oauth.reddit.com/r/saphanagraph/comments/{i}',headers=headers).json()\n",
    "    comments[f\"{i}\"] = []\n",
    "    def parse_comments(data):\n",
    "        for j in data:\n",
    "            comments[f\"{i}\"].append(j['data']['body'])\n",
    "            if j['data']['replies'] != '':\n",
    "                parse_comments(j['data']['replies']['data']['children'])\n",
    "    if(len(comment_data[1]['data']['children'])):\n",
    "        parse_comments(comment_data[1]['data']['children'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3047a-073d-485c-a442-2a6b0459b6d3",
   "metadata": {},
   "source": [
    "Now generate the vector embeddings for the comments and create a pandas dataframe with the data extracted from the reddit, and their vector embeddings. Before sending the data to model we would anonymize the data and then Send the data if content has positive intention will save it as embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03394243-d854-4085-ba84-9c0d07b2c6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def user_intention(query) -> str:\n",
    "\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "    response =  llm.invoke(\"Classify the following query in one word as negative or positive : \" + query).content\n",
    "    if \"negative\" in response.lower():\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "from gen_ai_hub.proxy.native.openai import embeddings\n",
    "\n",
    "def get_embedding(input, model=\"text-embedding-ada-002\") -> str:\n",
    "    response = embeddings.create(\n",
    "      model_name=model,\n",
    "      input=input\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d3a54d-3c8a-4e65-be9f-d3d67008133f",
   "metadata": {},
   "source": [
    "Here we defined a user_intention() function that classifies data from reddit into positive and negative data. \n",
    "\n",
    "Anonymizing content and removing content that has negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f59e3a-5201-4917-9e44-70ecd3c39dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install presidio-anonymizer\n",
    "import pandas as pd\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\n",
    "\n",
    "analyzer = AnalyzerEngine(log_decision_process=False)\n",
    "batch_analyzer = BatchAnalyzerEngine(analyzer_engine=analyzer)\n",
    "\n",
    "extracted_data = []\n",
    "for post in enumerate(reddit_data):\n",
    "    post_content = post[1]['data']['selftext']\n",
    "    post_content = reddit_data[0]['data']['selftext']\n",
    "    anonymizer_results = analyzer.analyze(text=post_content, entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"IP_ADDRESS\"], language=\"en\", return_decision_process=True)\n",
    "    for i in anonymizer_results:\n",
    "        post_content = post_content.replace(i.entity_value, i.anonymized_value)\n",
    "    if user_intention(post_content):\n",
    "        row = [post[0]+1]\n",
    "        row.append(post[1]['data']['title'])\n",
    "        row.append(post_content)\n",
    "        row.append(str(comments[post[1]['data']['id']]))\n",
    "        row.append(str(get_embedding(row[1])))\n",
    "        extracted_data.append(row)\n",
    "# df = pd.DataFrame(extracted_data)\n",
    "# df.columns = [\"ID\", \"Title\", \"Post\", \"Comments\", \"Post-Vector\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ecc60-8d0e-4688-a24b-6a35ae487b82",
   "metadata": {},
   "source": [
    "You might have to restart your notebook after running the above code. Once you restart the notebook, start again from connecting to the reddit API.\n",
    "\n",
    "In the above code, we extracted the data from reddit and stored it in a dataframe. We used a batch_analyzer and batch_anonymizer to remove sensitive data from the data coming from reddit.\n",
    "\n",
    "\n",
    "Upload the data to your HANA vector storage as we saw in the previous section.\n",
    "\n",
    "Add your credentials to create your connectionContext\n",
    "\n",
    "Replace TABLENAME with a new table that you want to create. **Make sure that you use only uppercase letters and numbers in the tablename.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56769c-415b-462a-8056-4a1712d1044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml import ConnectionContext\n",
    "from hana_ml.dataframe import create_dataframe_from_pandas\n",
    "\n",
    "cc= ConnectionContext(\n",
    "    address='<host>',\n",
    "    port='443',\n",
    "    user='<user>',\n",
    "    password='<password>',\n",
    "    encrypt=True\n",
    "    )\n",
    "\n",
    "\n",
    "cursor = cc.connection.cursor()\n",
    "sql_command = '''CREATE TABLE TABLENAME(ID BIGINT, TITLE NCLOB, TEXT NCLOB, COMMENT NCLOB, VECTOR_STR REAL_VECTOR);'''\n",
    "cursor.execute(sql_command)\n",
    "cursor.close()\n",
    "\n",
    "# extracted_data contains the data to be inserted into the table\n",
    "cursor = cc.connection.cursor()\n",
    "sql_insert = 'INSERT INTO TABLENAME(ID, TITLE, TEXT, COMMENT, VECTOR_STR) VALUES (?,?,?,?,TO_REAL_VECTOR(?))'\n",
    "cursor.executemany(sql_insert,extracted_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7f75bb-3e48-4973-ac2b-1290787d23d2",
   "metadata": {},
   "source": [
    "\n",
    "The embeddings can be used to find texts that are similar to each other using techniques such as COSINE-SIMILARITY. You can use such techniques to find text data similar to your prompt from the HANA vector store.\n",
    "\n",
    "Now define a function to do similarity search in your data. This function takes a query as input and returns similar content from the table.\n",
    "\n",
    "Replace the TABLENAME with the name of your table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d754c658-8603-425c-9abd-47dd862b5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vector_search(query: str, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    if metric == 'L2DISTANCE':\n",
    "        sort = 'ASC'\n",
    "    else:\n",
    "        sort = 'DESC'\n",
    "    query_vector = get_embedding(query)\n",
    "    sql = '''SELECT TOP {k} \"TITLE\", \"TEXT\", \"COMMENT\"\n",
    "        FROM \"TABLENAME\"\n",
    "        ORDER BY \"{metric}\"(\"VECTOR_STR\", TO_REAL_VECTOR('{qv}')) {sort}'''.format(k=k, metric=metric, qv=query_vector, sort=sort)\n",
    "    hdf = cc.sql(sql)\n",
    "    df_context = hdf.head(k).collect()\n",
    "    return df_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aea3a9-1283-4942-b4fa-8908f8f12fde",
   "metadata": {},
   "source": [
    "We can define a prompt template to provide context to our prompts. Thus, when we pass a prompt to the model, the template will add the necessary context to the prompt so that better results are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a5e94-c037-4704-acd4-fd92e102ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate_fstring = \"\"\"\n",
    "You are an Python expert.\n",
    "You are provided multiple context items that are related to the prompt you have to answer.\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "from langchain.prompts import PromptTemplate\n",
    "promptTemplate = PromptTemplate.from_template(promptTemplate_fstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae837ab-c194-433d-be47-32eac49497e7",
   "metadata": {},
   "source": [
    "Here we have created a template for the prompts. It contains two variables, 'context' and 'query'. These variables will be replaced with the context and query in the upcoming steps \n",
    "\n",
    "Now we can define a function ask_llm() that queries a model using the above template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5a612-4bb4-45f8-9003-540e76cdcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "def ask_llm(query: str, metric='COSINE_SIMILARITY', k = 4) -> str:\n",
    "    context = ''\n",
    "    context = run_vector_search(query, metric, k)\n",
    "    prompt = promptTemplate.format(query=query, context=' '.join(context['TEXT'].astype('string')))\n",
    "    llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "    response = llm.invoke(prompt).content\n",
    "    print('Query: '+ query)\n",
    "    print('\\nResponse:')\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c00362-c55c-4816-ab42-d453062d41df",
   "metadata": {},
   "source": [
    "The above function appends the query and context into the template to create a prompt. Then that prompt is passed on to the LLM model and the response is printed.\n",
    "\n",
    "Here we are using gpt-4 model. Make sure that you have already created a deployment for the gpt-4 model in AI Launchpad\n",
    "\n",
    "Now we can test the function by passing the following query to generate results using retrieval augmented generation.ut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212e250-846e-4640-8c59-749803235fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I search for a word in a dictionary?\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac31b22-6fca-4aa0-9378-148cfc809c3a",
   "metadata": {},
   "source": [
    "We got a detailed output in contrast to the output for the same query without RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e273ba-5894-42d9-aab5-8fbc3f555048",
   "metadata": {},
   "source": [
    "We can see that the output generated is based on the comment that we posted in Reddit.\n",
    "\n",
    "**Outcome:** Through the integration of live Reddit data, Bob enhances the model's ability to provide up-to-date and relevant support to developers. By leveraging real-time insights and solutions shared by the community, the help portal powered by embeddings becomes even more valuable, addressing developers' queries with greater accuracy and efficiency.\n",
    "\n",
    "### Code generation\n",
    "\n",
    "We can make use of the LLM to generate code in python and other programming languages. In the following example, we make the model write a code to search for a word in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa676f-5025-4747-a981-6c27050c15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(proxy_model_name='gpt-4-32k')\n",
    "response = llm.invoke(\"write the python code to search for a word in a dictionary?\").content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc5d2c-893c-4b19-89bd-8a13e5d5a8c6",
   "metadata": {},
   "source": [
    "We can paste the generated code into a new cell here and run it to see its output\n",
    "\n",
    "Now we can use RAG to generate code. In the following example, we make use of RAG to generate code using the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8620247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary\n",
    "dictionary = {\"apple\": \"A fruit\", \"banana\": \"A yellow fruit\", \"cherry\": \"A small, round stone fruit\"}\n",
    "\n",
    "# Define the word to search\n",
    "word_to_search = \"banana\"\n",
    "\n",
    "# Check if the word is in the dictionary\n",
    "if word_to_search in dictionary:\n",
    "    print(f\"'{word_to_search}' found in the dictionary.\")\n",
    "else:\n",
    "    print(f\"'{word_to_search}' not found in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8799e-f948-4bdc-82d9-b224a328be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"write the python code to search for a word in a dictionary?\"\n",
    "response = ask_llm(query=query, k=4)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5458ec-f19d-459d-9dbb-3769c0ef14a4",
   "metadata": {},
   "source": [
    "We can paste the generated code into a new cell and run it to see its output.\n",
    "\n",
    "We can make use of the LLM to find the difference between the codes generated in both cases. Copy paste the code into the following prompt and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a165dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {\"apple\": \"a fruit\", \"banana\": \"a yellow fruit\", \"cherry\": \"a small, round, red fruit\"}\n",
    "\n",
    "# The word to search for\n",
    "word_to_find = \"banana\"\n",
    "\n",
    "if word_to_find in my_dict:\n",
    "    print(f\"'{word_to_find}' found in the dictionary with the meaning '{my_dict[word_to_find]}'.\")\n",
    "else:\n",
    "    print(f\"'{word_to_find}' not found in the dictionary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6413733e-d710-48eb-9198-688c17e634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_explain(code1, code2):\n",
    "    # Split the code into lines\n",
    "    lines1 = code1.strip().split('\\n')\n",
    "    lines2 = code2.strip().split('\\n')\n",
    "    \n",
    "    # Find differences in lines\n",
    "    diff_lines = []\n",
    "    for i, (line1, line2) in enumerate(zip(lines1, lines2), 1):\n",
    "        if line1 != line2:\n",
    "            diff_lines.append((i, line1, line2))\n",
    "    \n",
    "    if not diff_lines:\n",
    "        return \"Both code snippets are identical.\"\n",
    "    \n",
    "    # Generate difference report with explanations\n",
    "    report = \"Differences found:\\n\"\n",
    "    for line_num, line1, line2 in diff_lines:\n",
    "        report += f\"Line {line_num}:\\n\"\n",
    "        report += f\"  Code-1: {line1}\\n\"\n",
    "        report += f\"  Code-2: {line2}\\n\"\n",
    "        \n",
    "        # Explanation for the differences\n",
    "        if line_num == 2:  # Line with the dictionary definition\n",
    "            report += \"  Explanation: Variable names differ (dictionary vs my_dict).\\n\"\n",
    "        elif line_num == 6:  # Line with the word to search\n",
    "            report += \"  Explanation: Variable names differ (word_to_search vs word_to_find).\\n\"\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Define the two code snippets to compare\n",
    "code1 = '''\n",
    "query = \"\"\n",
    "dictionary = {\"apple\": \"A fruit\", \"banana\": \"A yellow fruit\", \"cherry\": \"A small, round stone fruit\"}\n",
    "word_to_search = \"banana\"\n",
    "\n",
    "if word_to_search in dictionary:\n",
    "    print(f\"'{word_to_search}' found in the dictionary.\")\n",
    "else:\n",
    "    print(f\"'{word_to_search}' not found in the dictionary.\")\n",
    "'''\n",
    "\n",
    "code2 = '''\n",
    "query = \"\"\n",
    "my_dict = {\"apple\": \"a fruit\", \"banana\": \"a yellow fruit\", \"cherry\": \"a small, round, red fruit\"}\n",
    "word_to_find = \"banana\"\n",
    "\n",
    "if word_to_find in my_dict:\n",
    "    print(f\"'{word_to_find}' found in the dictionary with the meaning '{my_dict[word_to_find]}'.\")\n",
    "else:\n",
    "    print(f\"'{word_to_find}' not found in the dictionary.\")\n",
    "'''\n",
    "\n",
    "# Compare the two code snippets\n",
    "result = compare_and_explain(code1, code2)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31459653-642d-41df-88b6-4612ae010b43",
   "metadata": {},
   "source": [
    "Here we can see how the codes differ from each other.\n",
    "\n",
    "**Outcome:** After generating the code, Bob tests the code and pushes it to github for testing and quality assurance. The effort to write code is reduced by a big margin by making use of the LLMs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
