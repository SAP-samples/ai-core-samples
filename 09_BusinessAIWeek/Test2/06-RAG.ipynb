{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Retrieval for a Retrieval Augmented Generation (RAG) Use Case\n",
    "\n",
    "Now that you have all your context information stored in the SAP HANA Cloud Vector Store, you can start asking the LLM questions about the orchestration service of Generative AI Hub. This time, the model will not respond from its knowledge base‚Äîwhat it learned during training‚Äîbut instead, the retriever will search for relevant context information in your vector database and send the appropriate text chunk to the LLM to review before responding.\n",
    "\n",
    "üëâ Change the `LLM_DEPLOYMENT_ID` in [variables.py](variables.py) to your deployment ID from exercise [01-explore-genai-hub](01-explore-genai-hub.md). For that go to **SAP AI Launchpad** application and navigate to **ML Operations** > **Deployments**.\n",
    "\n",
    "‚òùÔ∏è The `LLM_DEPLOYMENT_ID` is the deployment ID of the chat model you want to use e.g. **gpt-4o-mini**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import init_env\n",
    "# import variables\n",
    "\n",
    "#init_env.set_environment_variables()\n",
    "\n",
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "from hdbcli import dbapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are again connecting to our shared SAP HANA Cloud Vector Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # connect to HANA instance\n",
    "# connection = init_env.connect_to_hana_db()\n",
    "# connection.isconnected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hana_connection():\n",
    "    conn = dbapi.connect(\n",
    "        address='ec41b786-96de-467b-9ff5-db725945f89c.hna0.prod-us10.hanacloud.ondemand.com',\n",
    "        port='443',\n",
    "        user='DBADMIN',\n",
    "        password='9hEW4UK86Fdt',\n",
    "        encrypt=True\n",
    "    )\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://israel-fsvdxbsq.authentication.eu11.hana.ondemand.com/oauth/token\n",
      "sb-49ec08a9-d325-4480-9418-ad8801558203!b28574|aicore!b18\n",
      "96d2ba69-3289-4190-ad82-c174e50f9f17$8C_adlgCYD6AscPgIKtLXJkIj1AL6i8p9Opw1JJZ0o8=\n",
      "https://api.ai.prodeuonly.eu-central-1.aws.ml.hana.ondemand.com/v2\n",
      "grounding\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "# Inline credentials\n",
    "with open('creds.json') as f:\n",
    "    credCF = json.load(f)\n",
    " \n",
    "# Set environment variables\n",
    "def set_environment_vars(credCF):\n",
    "    env_vars = {\n",
    "        'AICORE_AUTH_URL': credCF['url'] + '/oauth/token',\n",
    "        'AICORE_CLIENT_ID': credCF['clientid'],\n",
    "        'AICORE_CLIENT_SECRET': credCF['clientsecret'],\n",
    "        'AICORE_BASE_URL': credCF[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\",\n",
    "        'AICORE_RESOURCE_GROUP': \"grounding\"\n",
    "    }\n",
    " \n",
    "    for key, value in env_vars.items():\n",
    "        os.environ[key] = value\n",
    "        print(value)\n",
    " \n",
    "# Create AI Core client instance\n",
    "def create_ai_core_client(credCF):\n",
    "    set_environment_vars(credCF)  # Ensure environment variables are set\n",
    "    return AICoreV2Client(\n",
    "        base_url=os.environ['AICORE_BASE_URL'],\n",
    "        auth_url=os.environ['AICORE_AUTH_URL'],\n",
    "        client_id=os.environ['AICORE_CLIENT_ID'],\n",
    "        client_secret=os.environ['AICORE_CLIENT_SECRET'],\n",
    "        resource_group=os.environ['AICORE_RESOURCE_GROUP']\n",
    "    )\n",
    " \n",
    "ai_core_client = create_ai_core_client(credCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DEPLOYMENT_ID = \"d1ae8a45e3d60115\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference which embedding model, DB connection and table to use\n",
    "embeddings = OpenAIEmbeddings(deployment_id=EMBEDDING_DEPLOYMENT_ID)\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=get_hana_connection(), table_name=\"EMBEDDINGS_CODEJAM_\"+\"Testing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step you are defining which LLM to use during the retrieving process. You then also assign which database to retrieve information from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_DEPLOYMENT_ID = \"d6dd6e483c92f88c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to fix with the name mentioned in deployement id\n",
    "# Define which model to use\n",
    "chat_llm = ChatOpenAI(deployment_id=LLM_DEPLOYMENT_ID)\n",
    "\n",
    "# Create a retriever instance of the vector store\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Instead of sending the query directly to the LLM, you will now create a `RetrievalQA` instance and pass both the LLM and the database to be used during the retrieval process. Once set up, you can send your query to the `Retriever`.\n",
    "\n",
    "üëâ Try out different queries. Feel free to ask anything you'd like to know about the Models that are available in Generative AI Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is data masking in the orchestration service?',\n",
       " 'result': \"I don't know.\",\n",
       " 'source_documents': [Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-02-02T21:04:28+00:00', 'title': 'Orchestration Service | Generative AI hub SDK 4.1.1 documentation', 'moddate': '2025-02-02T21:04:28+00:00', 'source': 'documents\\\\Orchestration_Service_Generative_AI_hub_SDK.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='/Examples/Orchestration Service\\nOrchestration Service\\nThis notebook demonstrates how to use the SDK to interact with the Orchestration\\nService, enabling the creation of AI-driven workflows by seamlessly integrating various\\nmodules, such as templating, large language models (LLMs), data masking and content\\nfiltering. By leveraging these modules, you can build complex, automated workflows that\\nenhance the capabilities of your AI solutions. For more details on configuring and using'),\n",
       "  Document(metadata={'producer': 'Skia/PDF m131', 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36', 'creationdate': '2025-02-02T21:04:28+00:00', 'title': 'Orchestration Service | Generative AI hub SDK 4.1.1 documentation', 'moddate': '2025-02-02T21:04:28+00:00', 'source': 'documents\\\\Orchestration_Service_Generative_AI_hub_SDK.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html 14/14')]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the QA instance to query llm based on custom documents\n",
    "qa = RetrievalQA.from_llm(llm=chat_llm, retriever=retriever, return_source_documents=True)\n",
    "\n",
    "# Send query\n",
    "query = \"What is data masking in the orchestration service?\"\n",
    "\n",
    "answer = qa.invoke(query)\n",
    "display(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Skia/PDF m131',\n",
       " 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
       " 'creationdate': '2025-02-02T21:04:28+00:00',\n",
       " 'title': 'Orchestration Service | Generative AI hub SDK 4.1.1 documentation',\n",
       " 'moddate': '2025-02-02T21:04:28+00:00',\n",
       " 'source': 'documents\\\\Orchestration_Service_Generative_AI_hub_SDK.pdf',\n",
       " 'total_pages': 14,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Examples/Orchestration Service\n",
      "Orchestration Service\n",
      "This notebook demonstrates how to use the SDK to interact with the Orchestration\n",
      "Service, enabling the creation of AI-driven workflows by seamlessly integrating various\n",
      "modules, such as templating, large language models (LLMs), data masking and content\n",
      "filtering. By leveraging these modules, you can build complex, automated workflows that\n",
      "enhance the capabilities of your AI solutions. For more details on configuring and using\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'producer': 'Skia/PDF m131',\n",
       " 'creator': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',\n",
       " 'creationdate': '2025-02-02T21:04:28+00:00',\n",
       " 'title': 'Orchestration Service | Generative AI hub SDK 4.1.1 documentation',\n",
       " 'moddate': '2025-02-02T21:04:28+00:00',\n",
       " 'source': 'documents\\\\Orchestration_Service_Generative_AI_hub_SDK.pdf',\n",
       " 'total_pages': 14,\n",
       " 'page': 13,\n",
       " 'page_label': '14'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html 14/14\n"
     ]
    }
   ],
   "source": [
    "for document in answer['source_documents']:\n",
    "    display(document.metadata)   \n",
    "    print(document.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Go back to [05-store-embeddings-hana](05-store-embeddings-hana.ipynb) and try out different chunk sizes and/or different values for overlap. Store these chunks in a different table by adding a new variable to [variables.py](variables.py) and run this script again using the newly created table.\n",
    "\n",
    "[Next exercise](07-deploy-orchestration-service.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
