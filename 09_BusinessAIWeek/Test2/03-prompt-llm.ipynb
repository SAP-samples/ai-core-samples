{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your connection to Generative AI Hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First you need to assign the values from `generative-ai-codejam/.aicore-config.json` to the environmental variables. \n",
    "\n",
    "That way the Generative AI Hub [Python SDK](https://pypi.org/project/generative-ai-hub-sdk/) will connect to Generative AI Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ For the Python SDK to know which resource group to use, you also need to set the `resource group` in the [`variables.py`](variables.py) file to your own `resource group` (e.g. **team-01**) that you created in the SAP AI Launchpad in exercise [00-connect-AICore-and-AILaunchpad](000-connect-AICore-and-AILaunchpad.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install generative-ai-hub-sdk[all] hdbcli scipy pypdf --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://israel-fsvdxbsq.authentication.eu11.hana.ondemand.com/oauth/token\n",
      "sb-49ec08a9-d325-4480-9418-ad8801558203!b28574|aicore!b18\n",
      "96d2ba69-3289-4190-ad82-c174e50f9f17$8C_adlgCYD6AscPgIKtLXJkIj1AL6i8p9Opw1JJZ0o8=\n",
      "https://api.ai.prodeuonly.eu-central-1.aws.ml.hana.ondemand.com/v2\n",
      "grounding\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "# Inline credentials\n",
    "with open('creds.json') as f:\n",
    "    credCF = json.load(f)\n",
    " \n",
    "# Set environment variables\n",
    "def set_environment_vars(credCF):\n",
    "    env_vars = {\n",
    "        'AICORE_AUTH_URL': credCF['url'] + '/oauth/token',\n",
    "        'AICORE_CLIENT_ID': credCF['clientid'],\n",
    "        'AICORE_CLIENT_SECRET': credCF['clientsecret'],\n",
    "        'AICORE_BASE_URL': credCF[\"serviceurls\"][\"AI_API_URL\"] + \"/v2\",\n",
    "        'AICORE_RESOURCE_GROUP': \"grounding\"\n",
    "    }\n",
    " \n",
    "    for key, value in env_vars.items():\n",
    "        os.environ[key] = value\n",
    "        print(value)\n",
    " \n",
    "# Create AI Core client instance\n",
    "def create_ai_core_client(credCF):\n",
    "    set_environment_vars(credCF)  # Ensure environment variables are set\n",
    "    return AICoreV2Client(\n",
    "        base_url=os.environ['AICORE_BASE_URL'],\n",
    "        auth_url=os.environ['AICORE_AUTH_URL'],\n",
    "        client_id=os.environ['AICORE_CLIENT_ID'],\n",
    "        client_secret=os.environ['AICORE_CLIENT_SECRET'],\n",
    "        resource_group=os.environ['AICORE_RESOURCE_GROUP']\n",
    "    )\n",
    " \n",
    "ai_core_client = create_ai_core_client(credCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import init_env\n",
    "# #import variables\n",
    "# #import importlib\n",
    "# #variables = importlib.reload(variables)\n",
    "\n",
    "# # TODO: You need to specify which model you want to use. In this case we are directing our prompt\n",
    "# # to the openAI API directly so you need to pick one of the GPT models. Make sure the model is actually deployed\n",
    "# # in genAI Hub. You might also want to chose a model that can also process images here already. \n",
    "# # E.g. 'gpt-4o-mini' or 'gpt-4o'\n",
    "# MODEL_NAME = 'gpt-4o'\n",
    "# # Do not modify the `assert` line below\n",
    "# assert MODEL_NAME!='', \"\"\"You should change the variable `MODEL_NAME` with the name of your deployed model (like 'gpt-4o-mini') first!\"\"\"\n",
    "\n",
    "# #init_env.set_environment_variables()\n",
    "# # Do not modify the `assert` line below \n",
    "# assert variables.RESOURCE_GROUP!='', \"\"\"You should change the value assigned to the `RESOURCE_GROUP` in the `variables.py` file to your own resource group first!\"\"\"\n",
    "# print(f\"Resource group is set to: {variables.RESOURCE_GROUP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt an LLM with Generative AI Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The underlying architecture of a Large Language Model (LLM) is typically based on the Transformer model. The key components of this architecture include:\n",
       "\n",
       "1. **Attention Mechanism**: Specifically, self-attention allows the model to weigh the significance of different words relative to one another, enabling it to capture contextual relationships effectively.\n",
       "\n",
       "2. **Layers**: The Transformer consists of multiple stacked layers, each containing two main sub-layers: a multi-head self-attention mechanism and a feed-forward neural network.\n",
       "\n",
       "3. **Positional Encoding**: Since Transformers do not have a built-in sense of word order, positional encodings are added to input embeddings to retain the sequence information.\n",
       "\n",
       "4. **Training Objective**: LLMs are usually trained using unsupervised learning techniques, such as predicting the next word in a sequence (language modeling).\n",
       "\n",
       "This architecture allows LLMs to process and generate human-like text by understanding context, relationships, and nuances within language."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "from IPython.display import Markdown\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o-mini\", messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding roles\n",
    "Most LLMs have the roles `system`, `assistant` (GPT) or `model` (Gemini) and `user` that can be used to steer the models response. In the previous step you only used the role `user` to ask your question. \n",
    "\n",
    "üëâ Try out different `system` messages to change the response. You can also tell the model to not engage in smalltalk or only answer questions on a certain topic. Then try different user prompts as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "An LLM, hmm, based on transformer architecture it is. Attention mechanisms, self-attention layers, and feedforward networks, it employs. Training on vast data, it learns patterns and context. Simple, yet powerful, yes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        # TODO try changing the system prompt\n",
    "        \"content\": \"Speak like Yoda from Star Wars.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is the underlying model architecture of an LLM? Explain it as short as possible.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o-mini\", messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Also try to have it speak like a pirate.\n",
    "\n",
    "üëâ Now let's be more serious! Tell it to behave like an SAP consultant talking to AI Developers.\n",
    "\n",
    "üëâ Ask it to behave like an SAP Consultant talking to ABAP Developers and to make ABAP comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucinations\n",
    "üëâ Run the following question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Data masking in the context of orchestration services typically involves the processes and techniques used to obscure specific data within a dataset, thereby protecting sensitive information while retaining its usability for analytics, development, or testing purposes. Although SAP provides various orchestration services (like SAP Data Intelligence, SAP Process Orchestration, etc.), the fundamental principles of data masking generally apply across platforms.\n",
       "\n",
       "Here's an overview of how data masking might work within an orchestration service:\n",
       "\n",
       "1. **Definition of Sensitivity**: Identify the data elements that need to be masked based on business rules and compliance requirements. This may include personal information such as names, identification numbers, financial data, etc.\n",
       "\n",
       "2. **Masking Techniques**: Various techniques can be employed, including:\n",
       "   - **Static Data Masking**: This replaces the sensitive data with masked values and is typically used in non-production environments.\n",
       "   - **Dynamic Data Masking**: This keeps the original data intact in the backend, but provides masked values to users based on their roles or permissions.\n",
       "   - **Tokenization**: Sensitive data is replaced with non-sensitive tokens that can represent the original data without exposing it.\n",
       "   - **Shuffling**: Rearranging the values within a column or dataset so that the actual data is not easily identifiable.\n",
       "   - **Data Obfuscation**: Modifying the data format or appearance without significant data loss.\n",
       "\n",
       "3. **Integration within Orchestration Workflows**: The orchestration service can include data masking as part of its workflow processes. This means:\n",
       "   - **Data Extraction**: When extracting data from source systems, the orchestration service can apply masking logic to sensitive data before it is moved to another system or environment.\n",
       "   - **Data Transformation**: During data transformation processes (ETL - Extract, Transform, Load), masking can be incorporated into transformation rules.\n",
       "\n",
       "4. **Automation and Monitoring**: Data masking can be automated within workflows to ensure that any sensitive data flows are consistently masked. Additionally, monitoring can be set up to ensure compliance and traceability.\n",
       "\n",
       "5. **Auditing and Compliance**: Organizations often require audits to ensure that data masking procedures follow regulations (like GDPR, HIPAA, etc.). Orchestration services should include mechanisms for logging and monitoring access to sensitive data.\n",
       "\n",
       "6. **User Role Management**: Access controls can be integrated within the orchestration service to ensure that users view only masked data unless they have permission to see original data.\n",
       "\n",
       "7. **Performance Considerations**: It's important to maintain performance when applying data masking techniques, especially in high-transaction environments.\n",
       "\n",
       "To implement data masking effectively, organizations often combine these techniques to suit their specific needs, ensuring that they meet both security requirements and operational efficiency. Always consult relevant technical documentation specific to your SAP environment for detailed implementation approaches and tools available."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [\n",
    "    {   \"role\": \"system\", \n",
    "        \"content\": \"You are an SAP Consultant.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How does the data masking of the orchestration service work?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o-mini\", messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è Compare the response to [SAP Help - Generative AI Hub SDK](https://help.sap.com/doc/generative-ai-hub-sdk/CLOUD/en-US/_reference/orchestration-service.html). \n",
    "\n",
    "üëâ What did the model respond? Was it the truth or a hallucination?\n",
    "\n",
    "üëâ Which questions work well, which questions still do not work so well?\n",
    "\n",
    "# Use Multimodal Models\n",
    "\n",
    "Multimodal models can use different inputs such as text, audio and images. In Generative AI Hub on SAP AI Core you can access multiple multimodal models (e.g. `gpt-4o-mini`).\n",
    "\n",
    "üëâ If you have not deployed one of the gpt-4o-mini models in previous exercises, then go back to the model library and deploy a model that can also process images.\n",
    "\n",
    "üëâ Now run the code snippet below to get a description for [the image of the AI Foundation Architecture](documents/ai-foundation-architecture.png). These descriptions can then for example be used as alternative text for screen readers or other assistive tech.\n",
    "\n",
    "üëâ You can upload your own image and play around with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The image is a structured diagram titled \"AI Foundation on SAP BTP.\" It outlines various components related to AI services and management within the SAP Business Technology Platform. \n",
       "\n",
       "At the top, there are sections labeled \"AI Services,\" displaying four features: Document Processing, Recommendation, Machine Translation, and Generative AI Management, highlighted in a pink box. Under Generative AI Management, there are three subcategories: Toolset, Trust & Control, and Access.\n",
       "\n",
       "Beneath that, the \"AI Workload Management\" section includes Training and Inference, while the \"Business Data & Context\" segment features Vector Engine and Data Management in blue boxes.\n",
       "\n",
       "Further down, the \"Foundation Models\" area lists three categories: SAP built, Hosted, and Remote, with an additional category for Fine-tuned. At the bottom, there is a section titled \"Lifecycle Management\" in a blue box. The overall layout is organized and visually segmented for clarity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# get the image from the documents folder\n",
    "with open(\"documents/ai-foundation-architecture.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Describe the images as an alternative text.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o-mini\", messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text from images\n",
    "Nora loves bananabread and thinks recipes are a good example of how LLMs can also extract complex text from images, like from [a picture of a recipe of a bananabread](documents/bananabread.png). Try your own recipe if you like :)\n",
    "\n",
    "This exercise also shows how you can use the output of an LLM in other systems, as you can tell the LLM how to output information, for example in JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the ingredients and instructions extracted from the banana bread recipe into two separate JSON files.\n",
       "\n",
       "**Ingredients JSON:**\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"dry_ingredients\": {\n",
       "    \"all_purpose_flour\": \"260 g\",\n",
       "    \"sugar\": \"200 g\",\n",
       "    \"baking_soda\": \"6 g\",\n",
       "    \"salt\": \"3 g\"\n",
       "  },\n",
       "  \"wet_ingredients\": {\n",
       "    \"banana\": \"225 g\",\n",
       "    \"large_eggs\": \"2\",\n",
       "    \"vegetable_oil\": \"100 g\",\n",
       "    \"whole_milk\": \"55 g\",\n",
       "    \"vanilla_extract\": \"5 g\"\n",
       "  },\n",
       "  \"toppings\": {\n",
       "    \"chocolate_chips\": \"100 g\",\n",
       "    \"walnuts\": \"100 g (optional)\"\n",
       "  }\n",
       "}\n",
       "```\n",
       "\n",
       "**Instructions JSON:**\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"instructions\": [\n",
       "    \"Preheat oven to 180 ¬∞C\",\n",
       "    \"Mash banana in a bowl\",\n",
       "    \"Combine banana and other wet ingredients together in the same bowl\",\n",
       "    \"Mix all dry ingredients together in a separate bowl\",\n",
       "    \"Use a whisk to combine dry mixture into wet mixture until smooth\",\n",
       "    \"Pour mixture into a greased/buttered loaf pan\",\n",
       "    \"Place the pan into preheated oven on the middle rack and bake for about 60 minutes or until a toothpick comes out clean\",\n",
       "    \"Enjoy!\"\n",
       "  ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the image from the documents folder\n",
    "with open(\"documents/bananabread.png\", \"rb\") as image_file:\n",
    "    image_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Extract the ingredients and instructions in two different json files.\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }]\n",
    "        }]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o-mini\", messages=messages)\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next exercise](04-create-embeddings.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
