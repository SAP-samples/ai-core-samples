{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8fac17",
   "metadata": {},
   "source": [
    "# Kubeflow pipeline as AI Core workflow\n",
    "\n",
    "\n",
    "This notebook contains the code for the corresponding blog post.\n",
    "\n",
    "## Simple Kubeflow pipeline\n",
    "\n",
    "\n",
    "The pipeline will have two steps:\n",
    "1. Download the California housing dataset from a URL and store it as an artifact\n",
    "2. Use the stored dataset to train a Linear Regression model and make predictions which are also stored as an artifact\n",
    "\n",
    "Using the Kubeflow Python SDK `kfp`, we create Python functions for the two steps in our pipeline. The first function is called `make_step_download`. It uses pandas to read a csv file from a url and save it as an artifact. We say to Kubeflow, that this function will output a csv file with the parameters `output_csv: comp.OutputPath('CSV')`.\n",
    "\n",
    "The second function will receive a csv file as input `input_csv: comp.InputPath('CSV')` and output again a csv file with predictions `output_csv: comp.OutputPath('CSV'`. Again all necesary libraries are imported within the defined function. We train then a linear regression model on the dataset and save a csv file with predictions.\n",
    "\n",
    "After the definition of these functions, we use the `kfp.components.create_component_from_func` function to make these functions available as steps for our Kubeflow pipeline. We can also define the base docker images and which Python libraries should be installed.\n",
    "\n",
    "Afterwards we create a pipeline function where we use the before defined steps for our pipeline. We also define, that the output from our first step should be the input to the second step. This pipeline function can no be compiled to a YAML workflow file using the Kubeflow compiler `kfp.compiler.Compiler()`.\n",
    "\n",
    "This YAML file can be uploaded to a Kubeflow cluster and then the pipeline can be started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "774604ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.9'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efdff1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_download(output_csv: comp.OutputPath('CSV')):\n",
    "    import pandas as pd\n",
    "    url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    df.fillna(0,inplace=True)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    return None\n",
    "    \n",
    "def make_step_train(input_csv: comp.InputPath('CSV'), output_csv: comp.OutputPath('CSV')):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import pandas as pd\n",
    "    from joblib import load, dump\n",
    "    \n",
    "    df = pd.read_csv(input_csv)\n",
    "    X = df[['total_rooms','households', 'latitude','longitude','total_bedrooms','population','median_income']]\n",
    "    y_target = df['median_house_value'] \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_target, test_size=0.3)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_predict = model.predict(X_test) \n",
    "    rmse = mean_squared_error(y_predict, y_test, squared=False)\n",
    "    print(\"RMSE = \",rmse)\n",
    "    \n",
    "    df = pd.DataFrame(y_predict)\n",
    "    df.to_csv(output_csv)\n",
    "    return None\n",
    "\n",
    "\n",
    "step_download = kfp.components.create_component_from_func(\n",
    "    func=make_step_download,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4'])\n",
    "\n",
    "step_train = kfp.components.create_component_from_func(\n",
    "    func=make_step_train,\n",
    "    base_image='python:3.7',\n",
    "    packages_to_install=['pandas==1.1.4','sklearn'])\n",
    "\n",
    "def my_pipeline():\n",
    "    download_step = step_download()\n",
    "    train_step = step_train(download_step.outputs['output_csv'])\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path='pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a10b5",
   "metadata": {},
   "source": [
    "## Transform Kubeflow pipeline to AI Core Workflow\n",
    "\n",
    "In the following, we will show the changes we have to make to the YAML file to make it work with AI Core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0fe6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a before created docker image as base image\n",
    "step_download = kfp.components.create_component_from_func(\n",
    "    func=make_step_download,\n",
    "    base_image='docker.io/flxschneider/text-train:0.0.1')\n",
    "\n",
    "step_train = kfp.components.create_component_from_func(\n",
    "    func=make_step_train,\n",
    "    base_image='docker.io/flxschneider/text-train:0.0.1')\n",
    "\n",
    "def my_pipeline():\n",
    "    download_step = step_download()\n",
    "    train_step = step_train(download_step.outputs['output_csv'])\n",
    "    \n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=my_pipeline,\n",
    "    package_path='workflow.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9620173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "with open(\"workflow.yaml\", \"r\") as stream:\n",
    "      workflow = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7c19d2",
   "metadata": {},
   "source": [
    "The beginning of the workflow YAML file looks like this:\n",
    "\n",
    "`{'apiVersion': 'argoproj.io/v1alpha1',\n",
    " 'kind': 'Workflow',\n",
    " 'metadata': {'generateName': 'my-pipeline-',\n",
    "  'annotations': {'pipelines.kubeflow.org/kfp_sdk_version': '1.8.9',\n",
    "   'pipelines.kubeflow.org/pipeline_compilation_time': '2022-01-18T12:57:16.488742',\n",
    "   'pipelines.kubeflow.org/pipeline_spec': '{\"name\": \"My pipeline\"}'},\n",
    "  'labels': {'pipelines.kubeflow.org/kfp_sdk_version': '1.8.9'}},`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "955a663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Change kind from Workflow to WorkflowTemplate\n",
    "workflow[\"kind\"] = \"WorkflowTemplate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c871b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define a name for the workflow\n",
    "name = \"ca-housing-linreg\"\n",
    "workflow[\"metadata\"][\"name\"] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0344e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Add AI Core annotations, also define what kind of outputs we create\n",
    "AI_Core_annotations = {'scenarios.ai.sap.com/description': \"CA Housing linear regression\",\n",
    "  'scenarios.ai.sap.com/name': \"ca-housing-train-scenario\",\n",
    "  'executables.ai.sap.com/description': \"CA Housing linear regression\",\n",
    "  'executables.ai.sap.com/name': name,\n",
    "  'artifacts.ai.sap.com/make-step-download-output_csv.kind': \"dataset\",\n",
    "  'artifacts.ai.sap.com/make-step-train-output_csv.kind': \"dataset\"}\n",
    "\n",
    "workflow[\"metadata\"][\"annotations\"] = {**workflow[\"metadata\"][\"annotations\"],**AI_Core_annotations}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2af8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Add AI Core labels\n",
    "AI_Core_labels = {'scenarios.ai.sap.com/id': \"ca-housing\",\n",
    "    'executables.ai.sap.com/id': name,\n",
    "    'ai.sap.com/version': \"1.0.2\"}\n",
    "\n",
    "workflow[\"metadata\"][\"labels\"] = {**workflow[\"metadata\"][\"labels\"], **AI_Core_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29b56773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Add ImagePullSecret if docker image is not public\n",
    "workflow[\"spec\"][\"imagePullSecrets\"] = [{\"name\" : \"docker-registry-secret2\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18127a0e",
   "metadata": {},
   "source": [
    "With the changes, the file looks now like this:\n",
    "\n",
    "`{'apiVersion': 'argoproj.io/v1alpha1',\n",
    " 'kind': 'WorkflowTemplate',\n",
    " 'metadata': {'generateName': 'my-pipeline-',\n",
    "  'annotations': {'pipelines.kubeflow.org/kfp_sdk_version': '1.8.9',\n",
    "   'pipelines.kubeflow.org/pipeline_compilation_time': '2022-01-18T12:59:24.447573',\n",
    "   'pipelines.kubeflow.org/pipeline_spec': '{\"name\": \"My pipeline\"}',\n",
    "   'scenarios.ai.sap.com/description': 'CA Housing linear regression',\n",
    "   'scenarios.ai.sap.com/name': 'ca-housing-train-scenario',\n",
    "   'executables.ai.sap.com/description': 'CA Housing linear regression',\n",
    "   'executables.ai.sap.com/name': 'ca-housing-linreg',\n",
    "   'artifacts.ai.sap.com/make-step-download-output_csv.kind': 'dataset',\n",
    "   'artifacts.ai.sap.com/make-step-train-output_csv.kind': 'dataset'},\n",
    "  'labels': {'pipelines.kubeflow.org/kfp_sdk_version': '1.8.9',\n",
    "   'scenarios.ai.sap.com/id': 'ca-housing',\n",
    "   'executables.ai.sap.com/id': 'ca-housing-train-LinReg',\n",
    "   'ai.sap.com/version': '1.0.2'},\n",
    "  'name': 'ca-housing-linreg'},`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8318bc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'make-step-download-output_csv',\n",
       " 'path': '/tmp/outputs/output_csv/data',\n",
       " 'archive': {'none': {}},\n",
       " 'globalName': 'make-step-download-output_csv'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Add globalName to the output artifacts\n",
    "\n",
    "name = workflow[\"spec\"][\"templates\"][0][\"outputs\"][\"artifacts\"][0][\"name\"]\n",
    "workflow[\"spec\"][\"templates\"][0][\"outputs\"][\"artifacts\"][0][\"archive\"] = {\"none\":{}}\n",
    "workflow[\"spec\"][\"templates\"][0][\"outputs\"][\"artifacts\"][0][\"globalName\"] = name\n",
    "\n",
    "name = workflow[\"spec\"][\"templates\"][1][\"outputs\"][\"artifacts\"][0][\"name\"]\n",
    "workflow[\"spec\"][\"templates\"][1][\"outputs\"][\"artifacts\"][0][\"archive\"] = {\"none\":{}}\n",
    "workflow[\"spec\"][\"templates\"][1][\"outputs\"][\"artifacts\"][0][\"globalName\"] = name\n",
    "workflow[\"spec\"][\"templates\"][0][\"outputs\"][\"artifacts\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da0f36",
   "metadata": {},
   "source": [
    "We now add a template with the name `my-pipeline` which is also our entrypoint. There we define which steps should be done by our pipeline:\n",
    "\n",
    "`{'name': 'my-pipeline',\n",
    "  'steps': [{'name': 'make-step-download', \n",
    "            'template': 'make-step-download'},\n",
    "           {'name': 'make-step-train', \n",
    "            'template': 'make-step-train'}]}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c20b7106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'my-pipeline',\n",
       " 'steps': [[{'name': 'make-step-download', 'template': 'make-step-download'}],\n",
       "  [{'name': 'make-step-train', 'template': 'make-step-train'}]]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. We define the steps of the pipeline\n",
    "new = {'name': 'my-pipeline',\n",
    " 'steps': [[{'name': 'make-step-download', 'template': 'make-step-download'}],\n",
    "  [{'name': 'make-step-train', 'template': 'make-step-train'}]]}\n",
    "workflow[\"spec\"][\"templates\"][2] = new\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec48d8",
   "metadata": {},
   "source": [
    "# Save new workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e135e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"workflow.yaml\", \"w\") as f:\n",
    "    yaml.dump(workflow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710151c",
   "metadata": {},
   "source": [
    "This YAML file can now be uploaded to a github repository which is connected to a AI Core instance. When we create a docker image before and adjust the smaller changes to the YAML file, we can use the Kubeflow SDK to create workflows for AI Core."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
